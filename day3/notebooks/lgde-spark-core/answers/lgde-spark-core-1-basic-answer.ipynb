{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1교시 스파크 기본 명령어\n",
    "\n",
    "> 스파크의 기본 명령어와 구조에 대해 이해합니다\n",
    "\n",
    "## 목차\n",
    "* [1. 스파크를 통한 CSV 파일 읽기](#1.-스파크를-통한-CSV-파일-읽기)\n",
    "* [2. 스파크의 2가지 프로그래밍 방식 비교](#2.-스파크의-2가지-프로그래밍-방식-비교)\n",
    "* [3. 스파크를 통한 JSON 파일 읽기](#3.-스파크를-통한-JSON-파일-읽기)\n",
    "* [4. 뷰 테이블 생성 및 조회](#4.-뷰-테이블-생성-및-조회)\n",
    "* [5. 스파크 애플리케이션의 개념 이해](#5.-스파크-애플리케이션의-개념-이해)\n",
    "* [6. 스파크 UI](#6.-스파크-UI)\n",
    "* [7. M&M 초콜렛 분류 예제](#7.-M&M-초콜렛-분류-예제)\n",
    "* [8. 파이썬 vs 스파크](#8.-파이썬-vs-스파크)\n",
    "* [9. 스파크 데이터 API 의 특징](#9.-스파크-데이터-API-의-특징)\n",
    "* [참고자료](#참고자료)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 스파크를 통한 CSV 파일 읽기\n",
    "> Spark 3.0.1 버전을 기준으로 작성되었습니다. 스파크는 2.0 버전으로 업데이트 되면서 DataFrames 은 Datasets 으로 통합되었고, 기존의 RDD 에서 사용하던 연산 및 기능과 DataFrame 에서 사용하던 것 모두 사용할 수 있습니다. 스파크 데이터 모델은 RDD (Spark1.0) —> Dataframe(Spark1.3) —> Dataset(Spark1.6) 형태로 업그레이드 되었으나, 본문에서 일부 DataFrames 와 DataSets 가 거의 유사하여, 일부 혼용되어 사용되는 경우가 있을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/21 09:06:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://36066668d2b9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7face4336dc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.6\n",
      "spark.version: 3.1.2\n"
     ]
    }
   ],
   "source": [
    "# !which python\n",
    "!/opt/conda/bin/python --version\n",
    "\n",
    "print(\"spark.version: {}\".format((spark.version)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 스파크 사용 관련 팁\n",
    "\n",
    "#### 여러 줄의 코드 작성\n",
    "* python 코드의 경우 괄호로 (python) 묶으면 이스케이핑(\\) 하지 않아도 됩니다\n",
    "* sql 문의 경우  \"\"\"sql\"\"\" 으로 묶으면 이스케이핑(\\)하지 않아도 됩니다\n",
    "\n",
    "#### 데이터 출력 함수\n",
    "* DataFrame.show() - 기본 제공 함수이며, show(n=limit) 통하여 최대 출력을 직접 조정할 수 있으나, 한글 출력 시에 표가 깨지는 경우가 있습니다\n",
    "* display(DataFrame) - Ipython 함수이며, limit 출력을 위해서는 limit 를 걸어야 하지만, 한글 출력에도 깨지지 않습니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      "\n",
      "+------+--------+\n",
      "|emp_id|emp_name|\n",
      "+------+--------+\n",
      "|     1|엘지전자|\n",
      "|     2|엘지화학|\n",
      "+------+--------+\n",
      "\n",
      "+------+\n",
      "|emp_id|\n",
      "+------+\n",
      "|     1|\n",
      "|     2|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th><th>emp_name</th></tr>\n",
       "<tr><td>1</td><td>엘지전자</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+--------+\n",
       "|emp_id|emp_name|\n",
       "+------+--------+\n",
       "|     1|엘지전자|\n",
       "|     2|엘지화학|\n",
       "+------+--------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th></tr>\n",
       "<tr><td>1</td></tr>\n",
       "<tr><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+\n",
       "|emp_id|\n",
       "+------+\n",
       "|     1|\n",
       "|     2|\n",
       "+------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 파이썬 코드 여러 줄 작성\n",
    "json = (\n",
    "    spark\n",
    "    .read\n",
    "    .json(f\"{work_data}/tmp/simple.json\")\n",
    "    .limit(2)\n",
    ")\n",
    "\n",
    "## 스파크 SQL 여러 줄 작성\n",
    "json.createOrReplaceTempView(\"simple\")\n",
    "spark.sql(\"\"\"\n",
    "    select * \n",
    "    from simple\n",
    "\"\"\")\n",
    "\n",
    "json.printSchema()\n",
    "emp_id = json.select(\"emp_id\")\n",
    "\n",
    "## 표준 데이터 출력함수\n",
    "json.show()\n",
    "emp_id.show()\n",
    "\n",
    "## 노트북 출력함수 \n",
    "display(json)\n",
    "display(emp_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컨테이너 기반 노트북\n",
    "> 컨테이너 내부에 존재하는 파일 등에 대해 직접 접근이 가능합니다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|value   |\n",
      "+--------+\n",
      "|boto3   |\n",
      "|scrapy  |\n",
      "|selenium|\n",
      "|mrjob   |\n",
      "|pyspark |\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "count of word is 9\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strings = spark.read.text(f\"{home_jovyan}/requirements.txt\")\n",
    "strings.show(5, truncate=False)\n",
    "count = strings.count()\n",
    "print(\"count of word is {}\".format(count))\n",
    "\n",
    "strings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.column import Column\n",
    "\n",
    "assert(type(strings) == DataFrame)\n",
    "assert(type(strings.value) == Column) # 현재 strings 데이터프레임의 스키마에 value 라는 하나의 컬럼만 존재합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(strings) # 데이터프레임은 Structured API 를 통해 Row 타입의 레코드를 다루는 함수를 이용하고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(strings.value) # 컬럼은 컬럼과의 비교 혹은 포함된 문자열을 다루는 contains 같은 함수를 사용합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 스키마 추정 (Inference)\n",
    "\n",
    "#### 아무런 옵션을 주지 않는 경우 스파크가 알아서 컬럼 이름과 데이터 타입을 (string) 지정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n",
      "+----------+-----+------+\n",
      "|       _c0|  _c1|   _c2|\n",
      "+----------+-----+------+\n",
      "|    a_time|a_uid|  a_id|\n",
      "|1603645200|    1| login|\n",
      "|1603647200|    1|logout|\n",
      "|1603649200|    2| login|\n",
      "|1603650200|    2|logout|\n",
      "|1603653200|    2| login|\n",
      "|1603657200|    3| login|\n",
      "|1603659200|    3|logout|\n",
      "|1603660200|    4| login|\n",
      "|1603664200|    4|logout|\n",
      "|1603664500|    4| login|\n",
      "|1603666500|    5| login|\n",
      "|1603669500|    5|logout|\n",
      "|1603670500|    6| login|\n",
      "|1603673500|    7| login|\n",
      "|1603674500|    8| login|\n",
      "|1603675500|    9| login|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_access = spark.read.csv(f\"{work_data}/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 첫 번째 라인에 헤더가 포함되어 있는 경우 아래와 같이 header option 을 지정하면 컬럼 명을 가져올 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a_time: string (nullable = true)\n",
      " |-- a_uid: string (nullable = true)\n",
      " |-- a_id: string (nullable = true)\n",
      "\n",
      "+----------+-----+------+\n",
      "|    a_time|a_uid|  a_id|\n",
      "+----------+-----+------+\n",
      "|1603645200|    1| login|\n",
      "|1603647200|    1|logout|\n",
      "|1603649200|    2| login|\n",
      "|1603650200|    2|logout|\n",
      "|1603653200|    2| login|\n",
      "|1603657200|    3| login|\n",
      "|1603659200|    3|logout|\n",
      "|1603660200|    4| login|\n",
      "|1603664200|    4|logout|\n",
      "|1603664500|    4| login|\n",
      "|1603666500|    5| login|\n",
      "|1603669500|    5|logout|\n",
      "|1603670500|    6| login|\n",
      "|1603673500|    7| login|\n",
      "|1603674500|    8| login|\n",
      "|1603675500|    9| login|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_access = spark.read.option(\"header\", \"true\").csv(f\"{work_data}/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inferSchema 옵션으로 데이터 값을 확인하고 스파크가 데이터 타입을 추정하게 할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a_time: integer (nullable = true)\n",
      " |-- a_uid: integer (nullable = true)\n",
      " |-- a_id: string (nullable = true)\n",
      "\n",
      "+----------+-----+------+\n",
      "|    a_time|a_uid|  a_id|\n",
      "+----------+-----+------+\n",
      "|1603645200|    1| login|\n",
      "|1603647200|    1|logout|\n",
      "|1603649200|    2| login|\n",
      "|1603650200|    2|logout|\n",
      "|1603653200|    2| login|\n",
      "|1603657200|    3| login|\n",
      "|1603659200|    3|logout|\n",
      "|1603660200|    4| login|\n",
      "|1603664200|    4|logout|\n",
      "|1603664500|    4| login|\n",
      "|1603666500|    5| login|\n",
      "|1603669500|    5|logout|\n",
      "|1603670500|    6| login|\n",
      "|1603673500|    7| login|\n",
      "|1603674500|    8| login|\n",
      "|1603675500|    9| login|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_access = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{work_data}/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>1. [기본]</font> \"data/flight-data/csv/2010-summary.csv\" 파일의 스키마와 데이터 10건을 출력하세요\n",
    "\n",
    "<details><summary>[정답] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df1 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/flight-data/csv/2010-summary.csv\")\n",
    ")\n",
    "    \n",
    "df1.printSchema()\n",
    "df1.show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "|    United States|          Singapore|   25|\n",
      "|    United States|            Grenada|   54|\n",
      "|       Costa Rica|      United States|  477|\n",
      "|          Senegal|      United States|   29|\n",
      "|    United States|   Marshall Islands|   44|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df1 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{work_data}/flight-data/csv/2010-summary.csv\")\n",
    ")\n",
    "\n",
    "df1.printSchema()\n",
    "df1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 스파크의 2가지 프로그래밍 방식 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하나. 구조화된 API 호출을 통해 데이터를 출력하는 방법\n",
    "> 출력 시에 bigint 값인 날짜는 아래와 같이 from_unixtime 및 to_timestamp 함수를 통해 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "| Arrival_Time|    String_Datetime|\n",
      "+-------------+-------------------+\n",
      "|1424686735090|2015-02-23 19:18:55|\n",
      "|1424686735292|2015-02-23 19:18:55|\n",
      "|1424686735500|2015-02-23 19:18:55|\n",
      "|1424686735691|2015-02-23 19:18:55|\n",
      "|1424686735890|2015-02-23 19:18:55|\n",
      "+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_timestamp, to_date, col, lit\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", \"true\").json(f\"{work_data}/activity-data\")\n",
    "\n",
    "# 구조화된 API 를 통한 구문\n",
    "timestamp = df.select(\n",
    "    \"Arrival_Time\",\n",
    "    to_timestamp(from_unixtime(col('Arrival_Time') / lit(1000)), 'yyyy-MM-dd HH:mm:ss').alias('String_Datetime')\n",
    ")\n",
    "timestamp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 둘. 표현식 형식으로 그대로 사용하여 출력하는 방법\n",
    "> 컬럼(col) 혹은 함수(concat 등)를 직접 사용하는 방식을 **구조화된 API** 를 사용한다고 말하고 SQL 구문으로 표현하는 방식을 **SQL 표현식**을 사용한다고 말합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "| Arrival_Time|    String_Datetime|\n",
      "+-------------+-------------------+\n",
      "|1424686735090|2015-02-23 19:18:55|\n",
      "|1424686735292|2015-02-23 19:18:55|\n",
      "|1424686735500|2015-02-23 19:18:55|\n",
      "|1424686735691|2015-02-23 19:18:55|\n",
      "|1424686735890|2015-02-23 19:18:55|\n",
      "+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL Expression 통한 구문\n",
    "ts = df.selectExpr(\n",
    "    \"Arrival_Time\",\n",
    "    \"to_timestamp(from_unixtime(Arrival_Time / 1000), 'yyyy-MM-dd HH:mm:ss') as String_Datetime\"\n",
    ")\n",
    "ts.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>2. [기본]</font> \"data/activity-data\" 경로에 저장된 json 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 10건의 데이터를 출력하세요\n",
    "#### 3. 'Creation_Time' 컬럼을 년월일 포맷으로 'String_Creation_Date' 컬럼을 출력하세요\n",
    "> 단, Creation_Time 은 Arrival_Time 과 정밀도가 달라서 1000 이 아니라 `1000000000` 을 나누어 주어야 합니다\n",
    "\n",
    "<details><summary>[실습2] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/activity-data\")\n",
    ")\n",
    "    \n",
    "df2.printSchema()\n",
    "display(df2.limit(3))\n",
    "answer = df2.limit(3).selectExpr(\n",
    "    \"Creation_Time\",\n",
    "    \"to_timestamp(from_unixtime(Creation_Time / 1000000000), 'yyyy-MM-dd HH:mm:ss') as String_Creation_Date\"\n",
    ")\n",
    "answer.show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Arrival_Time</th><th>Creation_Time</th><th>Device</th><th>Index</th><th>Model</th><th>User</th><th>gt</th><th>x</th><th>y</th><th>z</th></tr>\n",
       "<tr><td>1424686735090</td><td>1424686733090638193</td><td>nexus4_1</td><td>18</td><td>nexus4</td><td>g</td><td>stand</td><td>3.356934E-4</td><td>-5.645752E-4</td><td>-0.018814087</td></tr>\n",
       "<tr><td>1424686735292</td><td>1424688581345918092</td><td>nexus4_2</td><td>66</td><td>nexus4</td><td>g</td><td>stand</td><td>-0.005722046</td><td>0.029083252</td><td>0.005569458</td></tr>\n",
       "<tr><td>1424686735500</td><td>1424686733498505625</td><td>nexus4_1</td><td>99</td><td>nexus4</td><td>g</td><td>stand</td><td>0.0078125</td><td>-0.017654419</td><td>0.010025024</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
       "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|           y|           z|\n",
       "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
       "|1424686735090|1424686733090638193|nexus4_1|   18|nexus4|   g|stand| 3.356934E-4|-5.645752E-4|-0.018814087|\n",
       "|1424686735292|1424688581345918092|nexus4_2|   66|nexus4|   g|stand|-0.005722046| 0.029083252| 0.005569458|\n",
       "|1424686735500|1424686733498505625|nexus4_1|   99|nexus4|   g|stand|   0.0078125|-0.017654419| 0.010025024|\n",
       "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|      Creation_Time|String_Creation_Date|\n",
      "+-------------------+--------------------+\n",
      "|1424686733090638193| 2015-02-23 19:18:53|\n",
      "|1424688581345918092| 2015-02-23 19:49:41|\n",
      "|1424686733498505625| 2015-02-23 19:18:53|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(f\"{work_data}/activity-data\")\n",
    ")\n",
    "\n",
    "df2.printSchema()\n",
    "display(df2.limit(3))\n",
    "answer = df2.limit(3).selectExpr(\n",
    "    \"Creation_Time\",\n",
    "    \"to_timestamp(from_unixtime(Creation_Time / 1000000000), 'yyyy-MM-dd HH:mm:ss') as String_Creation_Date\"\n",
    ")\n",
    "answer.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 스파크를 통한 JSON 파일 읽기\n",
    "\n",
    "> 추후에 학습하게 될 예정인 filter 및 groupBy 구문이 사용되고 있는데요, 조건을 통해 데이터를 줄이고(filter), 특정 컬럼별 집계(groupBy)를 위한 연산자입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|user| count|\n",
      "+----+------+\n",
      "|   a|646627|\n",
      "|   b|729705|\n",
      "|   g|733185|\n",
      "|   c|617035|\n",
      "|   h|618415|\n",
      "+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "json = spark.read.json(f\"{work_data}/activity-data\")\n",
    "users = json.filter(\"index > 100\").select(\"index\", \"user\").groupBy(\"user\").count()\n",
    "users.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>3. [기본]</font> \"data/activity-data\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 10건의 데이터를 출력하세요\n",
    "#### 3. index 가 10000 미만의 이용자('user')별 빈도수를 구하세요\n",
    "\n",
    "<details><summary>[실습3] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/activity-data\")\n",
    ")\n",
    "    \n",
    "df3.printSchema()\n",
    "answer = df3.filter(\"index < 10000\").groupBy(\"user\").count()\n",
    "answer.show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   a|20000|\n",
      "|   b|20000|\n",
      "|   g|20000|\n",
      "|   c|20000|\n",
      "|   h|20000|\n",
      "|   f|20000|\n",
      "|   e|19999|\n",
      "|   i|20000|\n",
      "|   d|20000|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(f\"{work_data}/activity-data\")\n",
    ")\n",
    "\n",
    "df3.printSchema()\n",
    "answer = df3.filter(\"index < 10000\").groupBy(\"user\").count()\n",
    "answer.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 뷰 테이블 생성 및 조회\n",
    "> 이미 생성된 데이터 프레임을 통해서 현재 세션에서만 조회 가능한 임시 뷰 테이블을 만들어 SQL 질의가 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:==============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|user| count|\n",
      "+----+------+\n",
      "|   e|767980|\n",
      "|   i|740227|\n",
      "|   f|736240|\n",
      "|   g|733185|\n",
      "|   b|729705|\n",
      "+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "users.createOrReplaceTempView(\"users\")\n",
    "spark.sql(\"select * from users where count is not null and count > 9000 order by count desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>4. [기본]</font> \"data/flight-data/json/2015-summary.json\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. `2015_summary` 라는 임시 테이블을 생성하세요\n",
    "#### 2. spark sql 구문을 이용하여 10 건의 데이터를 출력하세요\n",
    "\n",
    "<details><summary>[실습4] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df4 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "    \n",
    "df4.printSchema()\n",
    "answer = df4.createOrReplaceTempView(\"2015_summary\")\n",
    "spark.sql(\"select * from 2015_summary\").show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df4 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(f\"{work_data}/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "\n",
    "df4.printSchema()\n",
    "answer = df4.createOrReplaceTempView(\"2015_summary\")\n",
    "spark.sql(\"select * from 2015_summary\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON 파일을 읽는 여러가지 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스키마 확인 - 3가지 모두 동일한 결과를 얻을 수 있으며 편한 방식을 선택하시면 됩니다\n",
    "df = spark.read.format(\"json\").load(f\"{work_data}/flight-data/json/2015-summary.json\") # 미국 교통통계국이 제공하는 항공운항 데이터\n",
    "df.printSchema()\n",
    "\n",
    "df2 = spark.read.load(f\"{work_data}/flight-data/json/2015-summary.json\", format=\"json\")\n",
    "df2.printSchema()\n",
    "\n",
    "df3 = spark.read.json(f\"{work_data}/flight-data/json/2015-summary.json\")\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 스파크 애플리케이션의 개념 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 스파크에서 반드시 알아야 할 객체와 개념\n",
    "| 구분 | 설명 | 기타 |\n",
    "|---|---|---|\n",
    "| Application | 스파크 프레임워크를 통해 빌드한 프로그램. 전체 작업을 관리하는 Driver 와 Executors 상에서 수행되는 프로그램으로 구분합니다 | - |\n",
    "| SparkSession | 스파크의 모든 기능을 사용하기 위해 생성하는 객체 | - |\n",
    "| Job | 하나의 액션(save, collect 등)을 수행하기 위해 여러개의 타스크로 구성된 병렬처리 단위 | DAG 혹은 Spark Execution Plan |\n",
    "| Stage | 하나의 잡은 다수의 스테이지라는 것으로 구성되며, 하나의 스테이지는 다수의 타스크 들로 구성됩니다 | - |\n",
    "| Task | 스파크 익스큐터에 보내지는 하나의 작업 단위 | 하나의 Core 혹은 Partition 단위의 작업 |\n",
    "\n",
    "### 5.2 스파크의 변환(Transformation)과 액션(Action)\n",
    "| 구분 | 설명 | 기타 |\n",
    "|---|---|---|\n",
    "| Transformation | 원본 데이터의 변경 없이 새로운 데이터프레임을 생성하는 모든 작업을 말하며 모든 변환 작업들은 lazily evaluated 되며 lineage 를 유지합니다 | select, filter, join, groupBy, orderBy |\n",
    "| Action | 여태까지 지연된 변환 작업을 트리거링하는 동작을 말하며, 데이터의 조회 및 저장 등의 작업을 말합니다 | show, take, count, collect, save |\n",
    "\n",
    "> Lineage : 연속된 변환 파이프라인이 액션을 만나기 전까지의 모든 이력 혹은 히스토리 정보를 모두 저장하고 있는 객체를 말하며, 스파크는 이렇게 체인을 구성한 변환 작업을 통해 **쿼리 최적화**를 수행할 수 있으며, 데이터 불변성(Immutability)를 통해서 **내결함성(Fault Tolerance)**을 가질 수 있습니다.\n",
    "\n",
    "### 5.3 좁은 변환과 넓은 변환 - Narrow and Wide Transformation\n",
    "> 위에서 언급한 최적화(Optimization) 과정은 여러 오퍼레이션들을 다시 여러 스테이지로 쪼개고, 이러한 스테이지 들이 셔플링이 필요한지, 클러스터간의 데이터 교환이 필요한지 등을 결정하는 문제이며, 변환 작업은 크게 하나의 파티션 내에 수행이 가능한 **좁은 의존성(narrow dependencies)** 과 셔플링이 발생하여 클러스터 전체에 데이터 교환이 필요한 **넓은 의존성(wide dependencies)** 두 가지로 구분합니다\n",
    "\n",
    "![Transformation](images/transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 스파크 UI\n",
    "> Default 포트는 4040 이므로 http://localhost:4040 에 접속하여 앞에서 배웠던 Narrow, Wide Transformation DAG를 확인합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|value                            |\n",
      "+---------------------------------+\n",
      "|jupyter_contrib_nbextensions     |\n",
      "|jupyter_nbextensions_configurator|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Narrow Transformation\n",
    "strings = spark.read.text(f\"{home_jovyan}/requirements.txt\")\n",
    "jupyter = strings.filter(strings.value.contains(\"jupyter\"))\n",
    "jupyter.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|u_gender|count|\n",
      "+--------+-----+\n",
      "|여      |3    |\n",
      "|남      |6    |\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wide Transformation\n",
    "user = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{work_data}/tbl_user.csv\")\n",
    "count = user.groupBy(\"u_gender\").count()\n",
    "count.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Narrow | Wide |\n",
    "|---|---|\n",
    "|![narrow](images/narrow.png)|![wide](images/wide.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. M&M 초콜렛 분류 예제\n",
    "> [Learning Spark 2nd Edition](https://github.com/psyoblade/LearningSparkV2?organization=psyoblade&organization=psyoblade) 에서 제공하는 data bricks dataset 예제 가운데 미국의 주 별 M&M 초콜렛 판매량을 집계하는 예제를 작성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "+-----+------+-----+\n",
      "|State|Color |Count|\n",
      "+-----+------+-----+\n",
      "|TX   |Red   |20   |\n",
      "|NV   |Blue  |66   |\n",
      "|CO   |Blue  |79   |\n",
      "|OR   |Blue  |71   |\n",
      "|WA   |Yellow|93   |\n",
      "|WY   |Blue  |16   |\n",
      "|CA   |Yellow|53   |\n",
      "|WA   |Green |60   |\n",
      "|OR   |Green |71   |\n",
      "|TX   |Green |68   |\n",
      "|NV   |Green |59   |\n",
      "|AZ   |Brown |95   |\n",
      "|WA   |Yellow|20   |\n",
      "|AZ   |Blue  |75   |\n",
      "|OR   |Brown |72   |\n",
      "|NV   |Red   |98   |\n",
      "|WY   |Orange|45   |\n",
      "|CO   |Blue  |52   |\n",
      "|TX   |Brown |94   |\n",
      "|CO   |Red   |82   |\n",
      "+-----+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnm_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{work_data}/databricks/mnm_dataset.csv\")\n",
    "mnm_df.printSchema()\n",
    "mnm_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Total|\n",
      "+-----+------+-----+\n",
      "|CA   |Yellow|1807 |\n",
      "|WA   |Green |1779 |\n",
      "|OR   |Orange|1743 |\n",
      "|TX   |Green |1737 |\n",
      "|TX   |Red   |1725 |\n",
      "|CA   |Green |1723 |\n",
      "|CO   |Yellow|1721 |\n",
      "|CA   |Brown |1718 |\n",
      "|CO   |Green |1713 |\n",
      "|NV   |Orange|1712 |\n",
      "|TX   |Yellow|1703 |\n",
      "|NV   |Green |1698 |\n",
      "|AZ   |Brown |1698 |\n",
      "|WY   |Green |1695 |\n",
      "|CO   |Blue  |1695 |\n",
      "|NM   |Red   |1690 |\n",
      "|AZ   |Orange|1689 |\n",
      "|NM   |Yellow|1688 |\n",
      "|NM   |Brown |1687 |\n",
      "|UT   |Orange|1684 |\n",
      "|NM   |Green |1682 |\n",
      "|UT   |Red   |1680 |\n",
      "|AZ   |Green |1676 |\n",
      "|NV   |Yellow|1675 |\n",
      "|NV   |Blue  |1673 |\n",
      "|WA   |Red   |1671 |\n",
      "|WY   |Red   |1670 |\n",
      "|WA   |Brown |1669 |\n",
      "|NM   |Orange|1665 |\n",
      "|WY   |Blue  |1664 |\n",
      "|WA   |Yellow|1663 |\n",
      "|WA   |Orange|1658 |\n",
      "|CA   |Orange|1657 |\n",
      "|NV   |Brown |1657 |\n",
      "|CA   |Red   |1656 |\n",
      "|CO   |Brown |1656 |\n",
      "|UT   |Blue  |1655 |\n",
      "|AZ   |Yellow|1654 |\n",
      "|TX   |Orange|1652 |\n",
      "|AZ   |Red   |1648 |\n",
      "|OR   |Blue  |1646 |\n",
      "|OR   |Red   |1645 |\n",
      "|UT   |Yellow|1645 |\n",
      "|CO   |Orange|1642 |\n",
      "|TX   |Brown |1641 |\n",
      "|NM   |Blue  |1638 |\n",
      "|AZ   |Blue  |1636 |\n",
      "|OR   |Green |1634 |\n",
      "|UT   |Brown |1631 |\n",
      "|WY   |Yellow|1626 |\n",
      "|WA   |Blue  |1625 |\n",
      "|CO   |Red   |1624 |\n",
      "|OR   |Brown |1621 |\n",
      "|OR   |Yellow|1614 |\n",
      "|TX   |Blue  |1614 |\n",
      "|NV   |Red   |1610 |\n",
      "|CA   |Blue  |1603 |\n",
      "|WY   |Orange|1595 |\n",
      "|UT   |Green |1591 |\n",
      "|WY   |Brown |1532 |\n",
      "+-----+------+-----+\n",
      "\n",
      "Total Rows = 60\n",
      "+-----+------+-----+\n",
      "|State|Color |Total|\n",
      "+-----+------+-----+\n",
      "|CA   |Yellow|1807 |\n",
      "|CA   |Green |1723 |\n",
      "|CA   |Brown |1718 |\n",
      "|CA   |Orange|1657 |\n",
      "|CA   |Red   |1656 |\n",
      "|CA   |Blue  |1603 |\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# We use the DataFrame high-level APIs. Note\n",
    "# that we don't use RDDs at all. Because some of Spark's\n",
    "# functions return the same object, we can chain function calls.\n",
    "# 1. Select from the DataFrame the fields \"State\", \"Color\", and \"Count\"\n",
    "# 2. Since we want to group each state and its M&M color count,\n",
    "# we use groupBy()\n",
    "# 3. Aggregate counts of all colors and groupBy() State and Color\n",
    "# 4 orderBy() in descending order\n",
    "count_mnm_df = (mnm_df.select(\"State\", \"Color\", \"Count\") \\\n",
    ".groupBy(\"State\", \"Color\") \\\n",
    ".agg(count(\"Count\").alias(\"Total\")) \\\n",
    ".orderBy(\"Total\", ascending=False))\n",
    "# Show the resulting aggregations for all the states and colors;\n",
    "# a total count of each color per state.\n",
    "# Note show() is an action, which will trigger the above\n",
    "# query to be executed.\n",
    "count_mnm_df.show(n=60, truncate=False)\n",
    "print(\"Total Rows = %d\" % (count_mnm_df.count()))\n",
    "\n",
    "# While the above code aggregated and counted for all\n",
    "# the states, what if we just want to see the data for\n",
    "# a single state, e.g., CA?\n",
    "# 1. Select from all rows in the DataFrame\n",
    "# 2. Filter only CA state\n",
    "# 3. groupBy() State and Color as we did above\n",
    "# 4. Aggregate the counts for each color\n",
    "# 5. orderBy() in descending order\n",
    "# Find the aggregate count for California by filtering\n",
    "ca_count_mnm_df = (mnm_df.select(\"State\", \"Color\", \"Count\") \\\n",
    ".where(mnm_df.State == \"CA\") \\\n",
    ".groupBy(\"State\", \"Color\") \\\n",
    ".agg(count(\"Count\").alias(\"Total\")) \\\n",
    ".orderBy(\"Total\", ascending=False))\n",
    "# Show the resulting aggregation for California.\n",
    "# As above, show() is an action that will trigger the execution of the\n",
    "# entire computation.\n",
    "ca_count_mnm_df.show(n=10, truncate=False)\n",
    "# Stop the SparkSession\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>5. [기본]</font> \"data/tbl_user.csv\" 경로의 CSV 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. `user` 라는 임시 테이블을 생성하세요\n",
    "#### 3. spark sql 구문을 이용하여 10 건의 데이터를 출력하세요\n",
    "\n",
    "<details><summary>[실습5] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df5 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/tbl_user.csv\")\n",
    ")\n",
    "    \n",
    "df5.printSchema()\n",
    "answer = df5.createOrReplaceTempView(\"user\")\n",
    "spark.sql(\"select * from user\").show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- u_id: integer (nullable = true)\n",
      " |-- u_name: string (nullable = true)\n",
      " |-- u_gender: string (nullable = true)\n",
      " |-- u_signup: integer (nullable = true)\n",
      "\n",
      "+----+----------+--------+--------+\n",
      "|u_id|    u_name|u_gender|u_signup|\n",
      "+----+----------+--------+--------+\n",
      "|   1|    정휘센|      남|19700808|\n",
      "|   2|  김싸이언|      남|19710201|\n",
      "|   3|    박트롬|      여|19951030|\n",
      "|   4|    청소기|      남|19770329|\n",
      "|   5|유코드제로|      여|20021029|\n",
      "|   6|  윤디오스|      남|20040101|\n",
      "|   7|  임모바일|      남|20040807|\n",
      "|   8|  조노트북|      여|20161201|\n",
      "|   9|  최컴퓨터|      남|20201124|\n",
      "+----+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df5 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{work_data}/tbl_user.csv\")\n",
    ")\n",
    "\n",
    "df5.printSchema()\n",
    "answer = df5.createOrReplaceTempView(\"user\")\n",
    "spark.sql(\"select * from user\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>6. [기본]</font> \"data/tbl_purchase.csv\" 경로의 CSV 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. `purchase` 라는 임시 테이블을 생성하세요\n",
    "#### 3. selectExpr 구문 혹은 spark sql 구문을 이용하여 `p_time` 필드를 날짜 함수를 이용하여 식별 가능하도록 데이터를 출력하세요\n",
    "\n",
    "<details><summary>[실습6] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df6 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/tbl_purchase.csv\")\n",
    ")\n",
    "    \n",
    "df6.printSchema()\n",
    "answer = df6.createOrReplaceTempView(\"purchase\")\n",
    "spark.sql(\"select from_unixtime(p_time) as p_time from purchase\").show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- p_time: integer (nullable = true)\n",
      " |-- p_uid: integer (nullable = true)\n",
      " |-- p_id: integer (nullable = true)\n",
      " |-- p_name: string (nullable = true)\n",
      " |-- p_amount: integer (nullable = true)\n",
      "\n",
      "+-------------------+\n",
      "|             p_time|\n",
      "+-------------------+\n",
      "|2020-10-26 03:45:50|\n",
      "|2020-10-26 03:45:50|\n",
      "|2020-10-26 15:45:55|\n",
      "|2020-10-26 09:51:40|\n",
      "|2020-10-26 03:55:55|\n",
      "|2020-10-26 10:08:20|\n",
      "|2020-10-26 07:45:55|\n",
      "|2020-10-26 07:49:15|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df6 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{work_data}/tbl_purchase.csv\")\n",
    ")\n",
    "\n",
    "df6.printSchema()\n",
    "answer = df6.createOrReplaceTempView(\"purchase\")\n",
    "spark.sql(\"select from_unixtime(p_time) as p_time from purchase\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 파이썬 vs 스파크\n",
    "\n",
    "### 8.1 파이썬을 이용하여 1에서 100까지 더하는 함수를 계산합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = 0\n",
    "for number in range(1, 101, 1): result += number\n",
    "print(result)\n",
    "\n",
    "# 파이썬 3.0 에서는 reduce 함수를 사용할 수 있습니다\n",
    "from functools import reduce \n",
    "reduce(lambda x, y: x + y, range(101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 스파크 Structured API 를 통해서 1 ~ 100 까지 더하는 함수를 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5050\n",
      "5050\n"
     ]
    }
   ],
   "source": [
    "from operator import add  # 파이썬의 operator 의 add 함수를 그대로 사용합니다.\n",
    "sc = spark.sparkContext\n",
    "parallels = sc.parallelize((range(1, 101, 1))).reduce(add)  # 1 ~ 101 이전까지 1씩 증가하는 숫자를 분산객체인 RDD를 반드시 생성해야 여러 노드의 메모리에 객체가 생성됩니다.\n",
    "print(parallels)\n",
    "\n",
    "x = sc.parallelize((range(1, 101, 1))).reduce(lambda x,y: x+y)  # 파이썬 람다함수를 이용해서 익명함수를 직접 생성해서 전달해도 결과는 동일합니다\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 스파크 데이터 API 의 특징\n",
    "\n",
    "### 9.1 Untyped Dataset 연산자 (aka Dataframe operations)\n",
    "\n",
    "> 타입이 없다고 하면 잘 이해가 가지 않는데 자세한 설명은 추후에 하기로 하고 Java/Scala 와 같은 strong type 언어와는 다르게 type 에 대한 강한 제약 없이 기본적인 데이터 연산자들을 사용할 수 있다 정도로 이해하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# activity-data : 다양한 장치 (특히 가속도계 및 자이로 스코프)의 스마트 폰 및 스마트 워치 센서 판독 값으로 구성된 다양한 사람의 활동 데이터 집합입니다.\n",
    "df = spark.read.option(\"inferSchema\", \"true\").json(f\"{work_data}/activity-data/part-00079-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+-------------+-------------+------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|            x|            y|           z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+-------------+-------------+------------+\n",
      "|1424686735175|1424688581230073365|nexus4_2|   43|nexus4|   g|stand|-0.0025177002| -0.054229736| 0.025863647|\n",
      "|1424686735377|1424686733377625498|nexus4_1|   75|nexus4|   g|stand|-0.0039367676|   0.02507019| -0.01133728|\n",
      "|1424686735577|1424688581632874879|nexus4_2|  123|nexus4|   g|stand| 0.0017547607|-0.0093688965|0.0012969971|\n",
      "|1424686735776|1424686733780457529|nexus4_1|  155|nexus4|   g|stand| 0.0014038086|  0.014389038|-0.013473511|\n",
      "|1424686735979|1424686733981873545|nexus4_1|  195|nexus4|   g|stand|-0.0018005371|  0.004776001| 0.023910522|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+-------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 출력 시에 bigint 값인 날짜는 아래와 같이 from_unixtime 및 to_timestamp 함수를 통해 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----------+\n",
      "| Arrival_Time|    String_Datetime|String_Date|\n",
      "+-------------+-------------------+-----------+\n",
      "|1424686735175|2015-02-23 19:18:55| 2015-02-23|\n",
      "|1424686735377|2015-02-23 19:18:55| 2015-02-23|\n",
      "|1424686735577|2015-02-23 19:18:55| 2015-02-23|\n",
      "|1424686735776|2015-02-23 19:18:55| 2015-02-23|\n",
      "|1424686735979|2015-02-23 19:18:55| 2015-02-23|\n",
      "+-------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_timestamp, to_date\n",
    "timestamp = df.select(\n",
    "    \"Arrival_Time\",\n",
    "    to_timestamp(from_unixtime(col('Arrival_Time') / lit(1000)), 'yyyy-MM-dd HH:mm:ss').alias('String_Datetime'),\n",
    "    to_date(from_unixtime(col('Arrival_Time') / lit(1000)), 'yyyy-MM-dd HH:mm:ss').alias('String_Date')\n",
    ")\n",
    "timestamp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 구조화된 API (Structued API)\n",
    "\n",
    "#### Selecting Dataframe using structured APIs\n",
    "\n",
    "> 스파크 API 이용 시에 컬럼명은 대소문자를 구분하지 않는 것이 기본설정입니다. (***spark.sql.caseSensitive is set to false***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  Device|count|\n",
      "+--------+-----+\n",
      "|nexus4_2|39351|\n",
      "|nexus4_1|38637|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# 아래의 select 구문에서는 col(\"컬럼명\") 혹은 \"컬럼명\" 둘다 혼용이 가능합니다.\n",
    "df.filter(col(\"Index\") > 100).select(col(\"Arrival_time\"), col(\"Creation_Time\"), col(\"Device\")).groupBy(\"Device\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+--------+\n",
      "|concat(Arrival_time, Creation_Time)|Device  |\n",
      "+-----------------------------------+--------+\n",
      "|14246867355771424688581632874879   |nexus4_2|\n",
      "|14246867357761424686733780457529   |nexus4_1|\n",
      "|14246867359791424686733981873545   |nexus4_1|\n",
      "|14246867361811424686734183442148   |nexus4_1|\n",
      "|14246867363831424686734387513193   |nexus4_1|\n",
      "+-----------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"Index\") > 100).select(concat(\"Arrival_time\", \"Creation_Time\"), \"Device\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------+\n",
      "|Concated_Time            |Device  |\n",
      "+-------------------------+--------+\n",
      "|Arrival_timeCreation_Time|nexus4_2|\n",
      "|Arrival_timeCreation_Time|nexus4_1|\n",
      "|Arrival_timeCreation_Time|nexus4_1|\n",
      "|Arrival_timeCreation_Time|nexus4_1|\n",
      "|Arrival_timeCreation_Time|nexus4_1|\n",
      "+-------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 아래와 같이 structured api 를 통해서 복잡한 구문을 selectExpr 을 통해 좀 더 편하게 조회할 수 있습니다.\n",
    "df.filter(col(\"Index\") > 100).selectExpr(\"concat('Arrival_time', 'Creation_Time') as Concated_Time\", \"Device\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   a| 8082|\n",
      "|   b| 9121|\n",
      "|   g| 9165|\n",
      "|   c| 7713|\n",
      "|   h| 7730|\n",
      "|   f| 9203|\n",
      "|   e| 9599|\n",
      "|   i| 9253|\n",
      "|   d| 8122|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"index\") > 100).select(\"index\", \"user\").groupBy(\"user\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   a| 8082|\n",
      "|   b| 9121|\n",
      "|   g| 9165|\n",
      "|   c| 7713|\n",
      "|   h| 7730|\n",
      "|   f| 9203|\n",
      "|   e| 9599|\n",
      "|   i| 9253|\n",
      "|   d| 8122|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 대부분의 구문에서 표현식을 통해 처리할 수 있도록 내부적으로 2가지 방식에 대해 모두 구현되어 있습니다. \n",
    "df.filter(\"index > 100\").select(\"index\", \"user\").groupBy(\"user\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select 뿐만 아니라 filter 의 경우도 Expression 을 사용할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   a| 8082|\n",
      "|   b| 9121|\n",
      "|   g| 9165|\n",
      "|   c| 7713|\n",
      "|   h| 7730|\n",
      "|   f| 9203|\n",
      "|   e| 9599|\n",
      "|   i| 9253|\n",
      "|   d| 8122|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   a| 8082|\n",
      "|   b| 9121|\n",
      "|   g| 9165|\n",
      "|   c| 7713|\n",
      "|   h| 7730|\n",
      "|   f| 9203|\n",
      "|   e| 9599|\n",
      "|   i| 9253|\n",
      "|   d| 8122|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"index\") > 100).select(\"index\", \"user\").groupBy(\"user\").count().show()\n",
    "# 대부분의 구문에서 표현식을 통해 처리할 수 있도록 내부적으로 2가지 방식에 대해 모두 구현되어 있습니다. \n",
    "df.filter(\"index > 100\").select(\"index\", \"user\").groupBy(\"user\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 뷰 테이블 생성 및 활용\n",
    "\n",
    "#### JSON 파일을 이용하여 데이터프레임 생성하기\n",
    "> 임의의 JSON 파일로 부터 데이터프레임을 생성하고 집계를 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   a| 8082|\n",
      "|   b| 9121|\n",
      "|   g| 9165|\n",
      "|   c| 7713|\n",
      "|   h| 7730|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json = spark.read.json(f\"{work_data}/activity-data/part-00079-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\")\n",
    "users = json.filter(\"index > 100\").select(\"index\", \"user\").groupBy(\"user\").count()\n",
    "users.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 임시 뷰 테이블 생성하기\n",
    "\n",
    "> 이미 생성된 데이터 프레임을 통해서 현재 세션에서만 조회 가능한 임시 뷰 테이블을 만들어 SQL 질의가 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   e| 9599|\n",
      "|   i| 9253|\n",
      "|   f| 9203|\n",
      "|   g| 9165|\n",
      "|   b| 9121|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.createOrReplaceTempView(\"users\")\n",
    "spark.sql(\"select * from users where count is not null and count > 9000 order by count desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 글로벌 뷰 테이블 생성하기\n",
    "\n",
    "> 현재 생성된 세션 외에서도 글로벌한 뷰 테이블 생성도 가능하며, global_temp 데이터베이스에 생성되어 $SELECT * FROM\\ global\\_temp.people$ 과 같은 형식으로 조회가 가능합니다. 다만, 하이브 테이블의 개념과 달리 영구적인 테이블 형태가 아니기 때문에 현재 수행하는 작업에 대해서만 사용하기를 권장합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   a| 8082|\n",
      "|   b| 9121|\n",
      "|   g| 9165|\n",
      "|   c| 7713|\n",
      "|   h| 7730|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.dropGlobalTempView(\"global_users\")\n",
    "users.createGlobalTempView(\"global_users\")\n",
    "spark.sql(\"select * from global_temp.global_users\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   g| 9165|\n",
      "|   f| 9203|\n",
      "|   e| 9599|\n",
      "|   h| 7730|\n",
      "|   d| 8122|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newSession = \"\"\"\n",
    "public SparkSession newSession()\n",
    "Start a new session with isolated SQL configurations, temporary tables, registered functions are isolated, \n",
    "but sharing the underlying SparkContext and cached data.\n",
    "\"\"\"\n",
    "spark.newSession().sql(\"select * from global_temp.global_users\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 참고자료\n",
    "\n",
    "#### 1. [Spark Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "#### 2. [PySpark SQL Modules Documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)\n",
    "#### 3. <a href=\"https://spark.apache.org/docs/3.0.1/api/sql/\" target=\"_blank\">PySpark 3.0.1 Builtin Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
