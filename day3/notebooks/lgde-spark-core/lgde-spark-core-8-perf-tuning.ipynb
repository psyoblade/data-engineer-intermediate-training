{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7교시 스파크 애플리케이션 튜닝 및 최적화\n",
    "\n",
    "> 6장에서는 스파크가 어떻게 메모리 관리를 하고, 고급 API 를 통해서 데이터셋을 구성하는 지에 대해 학습했으며, 이번 장에서는 최적화를 위한 스파크 설정과, 조인 전략들을 살펴보고, 스파크 UI 를 통해 안좋은 영향을 줄 수 있는 것들에 대한 힌트를 얻고자 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/21 09:10:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://36066668d2b9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa4085afdc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.1 Optimizing and Tuning Spark for Efficiency\n",
    "> 스파크는 [튜닝](https://spark.apache.org/docs/latest/tuning.html)을 위한 다양한 설정을 제공하며, [설정](https://spark.apache.org/docs/latest/configuration.html)값을 통해 확인할 수 있습니다\n",
    "\n",
    "### 7.1.1 Viewing and Setting Apache Spark Configurations\n",
    "> 아래의 순서대로 스파크는 설정값을 읽어들이며, 가장 마지막에 변경된 값이 반영됩니다\n",
    "\n",
    "#### 1. 설치된 스파크 경로의 conf/spark-default.conf 파일을 생성 및 수정\n",
    "\n",
    "#### 2. 스파크 실행 시에 옵션을 지정하는 방법\n",
    "```bash\n",
    "$ spark-submit --conf spark.sql.shuffle.partitions=5 --conf \"spark.executor.memory=2g\" --class main.scala.chapter7.SparkConfig_7_1 jars/mainscala-chapter7_2.12-1.0.jar\n",
    "```\n",
    "\n",
    "#### 3. 스파크 코드 내에서 직접 지정하는 방법\n",
    "```scala\n",
    "SparkSession.builder\n",
    ".config(\"spark.sql.shuffle.partitions\", 5)\n",
    ".config(\"spark.executor.memroy\", \"2g\")\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.app.id', 'local-1629537003980')\n",
      "('spark.app.name', 'pyspark-shell')\n",
      "('spark.app.startTime', '1629537002952')\n",
      "('spark.driver.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true')\n",
      "('spark.driver.host', '36066668d2b9')\n",
      "('spark.driver.port', '36767')\n",
      "('spark.executor.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.sql.session.timeZone', 'Asia/Seoul')\n",
      "('spark.sql.warehouse.dir', 'file:/home/jovyan/work/lgde-spark-core/spark-warehouse/')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "# 파이스파크 내에서는 sparkContext 를 통해서 해당 정보를 가져올 수 있습니다\n",
    "def printConfigs(session):\n",
    "    for x in sorted(session.sparkContext.getConf().getAll()):\n",
    "        print(x)\n",
    "\n",
    "printConfigs(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+----------------------------------------------------------------+\n",
      "|key                                                      |value                                                           |\n",
      "+---------------------------------------------------------+----------------------------------------------------------------+\n",
      "|spark.sql.adaptive.advisoryPartitionSizeInBytes          |<value of spark.sql.adaptive.shuffle.targetPostShuffleInputSize>|\n",
      "|spark.sql.adaptive.coalescePartitions.enabled            |true                                                            |\n",
      "|spark.sql.adaptive.coalescePartitions.initialPartitionNum|<undefined>                                                     |\n",
      "|spark.sql.adaptive.coalescePartitions.minPartitionNum    |<undefined>                                                     |\n",
      "|spark.sql.adaptive.enabled                               |false                                                           |\n",
      "+---------------------------------------------------------+----------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SparkSQL의 경우 내부적으로 사용되는 설정값이 다르기 때문에 더 많은 정보가 출력됩니다\n",
    "spark.sql(\"SET -v\").select(\"key\", \"value\").where(\"key like '%spark.sql%'\").show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 스파크 UI 를 통해서도 확인이 가능합니다\n",
    "\n",
    "![spakr-ui](images/spark-ui.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    }
   ],
   "source": [
    "# 스파크 기본 설정 spark.sql.shuffle.partitions 값을 확인하고, 프로그램 상에서 변경 후 테스트 합니다\n",
    "num_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "mod_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", num_partitions)\n",
    "print(num_partitions, mod_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 Scaling Spark for Large Workloads\n",
    "\n",
    "#### 1. 정적 vs 동적 리소스 할당의 선택\n",
    "> CPU 및 Memory 사용을 애플리케이션에 따라 지정하는 정적 리소스 할당과 동적 리소스 할당은 처리해야 할 데이터의 특성에 따라 선택할 수 있으며, 환경설정을 다르게 구성해야 합니다.\n",
    "\n",
    "* 데이터의 크기가 일정하지 않고, 유동적\n",
    "* 특히 데이터의 크기가 고르지 않은 스트리밍 처리\n",
    "* 멀티테넌시 환경의 분석용 클러스터의 데이터 리소스 관리\n",
    "\n",
    "#### 2. 동적 리소스 할당 설정 가이드\n",
    "* 기본 설정은 false 이므로 아래의 값들에 대한 설정이 별도로 되어야 하며, REPL 환경에서 지원하지 않는 값들도 존재하므로, 프로그램을 통한 수정이 필요합니다\n",
    "```\n",
    "spark.dynamicAllocation.enabled true\n",
    "spark.dynamicAllocation.minExecutors 2\n",
    "spark.dynamicAllocation.schedulerBacklogTimeout 1m\n",
    "spark.dynamicAllocation.maxExecutors 20\n",
    "spark.dynamicAllocation.executorIdleTimeout 2min\n",
    "```\n",
    "* 아래의 과정을 통해 동적 리소스를 관리합니다\n",
    "  - 1. 스파크 드라이버가 클러스터 매니저에 2개(minExecutors)의 익스큐터를 요청합니다\n",
    "  - 2. 작업 큐의 백로그가 증가하여, 백로그 타임아웃(schedulerBacklogTimeout)이 발생하는 경우 새로운 익스큐터 요청이 발생합니다\n",
    "  - 3. 스케줄링 된 작업들이 1분 이상 지연되는 경우 드라이버는 새로운 익스큐터를 최대 20개(maxExecutors) 까지 요청합니다\n",
    "  - 4. 스파크 드라이버는 2분 이상 (executorIdleTimeout) 작업이 할당되지 않는 익스큐터 들을 종료시킵니다\n",
    "\n",
    "#### 3. 스파크 익스큐터의 메모리와 셔플 서비스의 설정 가이드\n",
    "![external-memory-layout](images/external-memory-layout.png)\n",
    "* 맵, 스필 그리고 병합 프로세스들이 I/O 부족에 따른 문제점을 갖지 않으며, 최종 셔플 파티션이 디스크에 저장되기 전에 버퍼 메모리를 확보할 수 있도록 설정을 아래와 같이 조정할 수 있습니다\n",
    "![spark-conf-io](images/spark-conf-io.png)\n",
    "\n",
    "#### 4. 스파크 병렬성을 최대화\n",
    "> 스파크가 데이터를 어떻게 저장소로부터 메모리에 적재하는지, 스파크에 있어서 파티션이 어떻게 활용되는지를 이해해야 합니다\n",
    "\n",
    "* 매 스테이지 마다 많은 타스크들이 존재하지만, 스파크는 기껏해야 코어당 작업당 하나의 스레드만 할당하며, 개별 타스크는 독립된 파티션 하나를 처리합니다.\n",
    "* 리소스 사용을 최적화하고, 병렬성을 최대화 하려면 익스큐터에 존재하는 코어수들 만큼 많은 파티션들이 존재해야 합니다. (유휴 코어를 두지 않기 위함)\n",
    "![figure.7-3](images/figure.7-3.png)\n",
    "\n",
    "\n",
    "#### 5. 파티션은 구성에 대한 이해와 재구성\n",
    "* 분산 저장소에 저장시에 구성되는 경우\n",
    "  - HDFS, S3 등의 저장소의 기본 파일블록의 크기는 64mb, 128mb 이며, 파일 크기가 작고 많아질 수록 파티션당 할당해야 하는 코어수가 모자라기 때문에 \"small file problem\" 을 피해야 합니다\n",
    "* 스파크의 셔플링을 통해 생성되는 경우\n",
    "  - 집계함수나 조인과 같은 Wide Transformation 과정에서 셔플링이 발생 (Network & Disk I/O 비용)\n",
    "  - 기본 셔플 파티션 수는 200개인데 작은 데이터집합이나, 스트리밍 워크로드 등에는 **충분히 많은 수이기 때문에 조정이 필요**합니다\n",
    "  - 최종 결과 테이블의 용량 및 사용 용도에 따라 의도적인 파티션 수를 조정할 수 있습니다 (repartition, coalesce)\n",
    "\n",
    "#### 질문과 답변\n",
    "* 대부분 dynamic allocation 을 쓰면 좋을거 아닐까?\n",
    "  - 워크로드가 예상된다면 동적할당은 필요없는 리소스 및 관리 비용이 더 들어가기 때문에 성능에 영향을 줄 수 있습니다\n",
    "* REPL 이 뭔가?\n",
    "  - Read-Evaluate-Print Loop 의 약자\n",
    "* dynamic allocation 은 수시로 변경할 수 없는가? 왜 그런가?\n",
    "* off-heap 이 좋으면 모두 off-heap 사용하지 왜 jvm 메모리를 이렇게나 많이 사용하는가?\n",
    "  - 자바에서 사용하는 구조화된 API의 장점과 네이티브 라이브러리의 데이터 송수신 및 읽고 쓰기의 장점을 모두 취하기 위함\n",
    "* execution vs storage 메모리의 비율을 어떻게 확인할 수 있는가? 오히려 삽질 아닌가?\n",
    "  - 직접 셋팅하기 보다는 관련 옵션을 조정하면서 튜닝합니다\n",
    "* spark 작업에서의 spill 절차는 무엇이고 왜 발생하며 어떻게 해결할 수 있는가?\n",
    "  - 스파크 익스큐터가 위의 각 레이어에 할당된 메모리를 모두 사용한 경우 디스크로 저장하는 경우를 Spill 이라고 합니다\n",
    "  - Disk I/O 는 성능에 큰 영향을 미치기 때문에 SSD 를 사용한다면 좋은 성능을 효과를 기대할 수 있습니다\n",
    "```\n",
    "operations, the shuffle will spill results to executors’ local disks at the location specified in spark.local.directory. Having performant SSD disks for this operation will boost the performance.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numDF = spark.range(1000).repartition(16)\n",
    "numDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Caching and Persistence of Data\n",
    "> cache() 와 persist() 는 거의 동일하지만, persist() 의 경우 persistent level 을 결정할 수 있습니다 (메모리, 디스크, 직렬화, 비직렬화 등)\n",
    "\n",
    "### 7.2.1 DataFrame.cache()\n",
    "* DataFrame 은 부분적으로 캐시가 가능하지만, 파티션은 그렇지 못 합니다. 예를 들어 8개의 파티션 중 4.5개 정도를 사용할 메모리가 있는 경우 4개의 파티션만 캐시됩니다\n",
    "  - 캐시되지 않은 데이터를 읽는 데에는 문제가 없지만, 모두 다시 계산되어야 하는 비용이 발생합니다\n",
    "* cache() 혹은 persist() 호출 시에 DataFrame 은 take(1) 같은 경우 첫 번째 파티션만 캐싱이 이루어지고, count() 같은 action 수행 시에는 모든 데이터가 캐싱이 된다는 점을 알고 있어야 합니다\n",
    "  - rdd.cache()는 persist(StorageLevel.MEMORY_ONLY) 로 \n",
    "  - df.cache()는 persist(StorageLevel.MEMORY_AND_DISK) 로 동작합니다\n",
    "\n",
    "![persist-storage](images/persist-storage.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed number is 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.911931276321411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 반복적으로 수행하는 경우 노트북 프로그램의 캐싱될 수 있기 때문에, 매번 다른 프로그램 수행을 위해서 랜덤 시드숫자를 매번 더해줍니다.\n",
    "import random\n",
    "seed = random.randint(1,100)\n",
    "print(\"seed number is {}\".format(seed))\n",
    "cached = spark.range(10 * 1000 * 1000 + seed).toDF(\"id\").withColumn(\"square\", expr(\"id * id\"))\n",
    "import time\n",
    "start = time.time()\n",
    "cached.cache() # 데이터를 캐싱\n",
    "cached.count() # Materialize the cache\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13873028755187988\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cached.count()\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 DataFrame.persist()\n",
    "![table.7-2](images/table.7-2.png)\n",
    "![figure.7-5](images/figure.7-5.png)\n",
    "\n",
    "* 테이블 캐시를 사용하는 경우도 cache() 와 동일한 결과를 보여줍니다\n",
    "![figure.7-5-1](images/figure.7-5-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed number is 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.837291955947876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 반복적으로 수행하는 경우 노트북 프로그램의 캐싱될 수 있기 때문에, 매번 다른 프로그램 수행을 위해서 랜덤 시드숫자를 매번 더해줍니다.\n",
    "import random\n",
    "seed = random.randint(1,100)\n",
    "print(\"seed number is {}\".format(seed))\n",
    "persisted = spark.range(10 * 1000 * 1000 + seed).toDF(\"id\").withColumn(\"square\", expr(\"id * id\"))\n",
    "import time\n",
    "start = time.time()\n",
    "from pyspark import StorageLevel\n",
    "persisted.persist(StorageLevel.DISK_ONLY) # 데이터를 캐싱\n",
    "persisted.count() # Materialize the cache\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16534972190856934\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "persisted.count()\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed number is 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.994640827178955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 반복적으로 수행하는 경우 노트북 프로그램의 캐싱될 수 있기 때문에, 매번 다른 프로그램 수행을 위해서 랜덤 시드숫자를 매번 더해줍니다.\n",
    "import random\n",
    "seed = random.randint(1,100)\n",
    "print(\"seed number is {}\".format(seed))\n",
    "table_cached = spark.range(10 * 1000 * 1000 + seed).toDF(\"id\").withColumn(\"square\", expr(\"id * id\"))\n",
    "table_cached.createOrReplaceTempView(\"square\")\n",
    "import time\n",
    "start = time.time()\n",
    "spark.sql(\"CACHE TABLE square\") # 데이터를 캐싱\n",
    "spark.sql(\"SELECT COUNT(1) FROM square\") # Materialize the cache\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014763832092285156\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "spark.sql(\"SELECT COUNT(1) FROM square\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 When to Cache and Persist\n",
    "> 대용량 테이블을 자주 쿼리하는 경우 혹은 변환에 활용되는 경우에 사용합니다\n",
    "* 기계학습 훈련 시와 같이 반복 적인 데이터프레임의 조회\n",
    "* ETL 데이터 파이프라인의 변환작업에 빈번하게 사용되는 공통 테이블의 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4 When Not to Cache and Persist\n",
    "> 너무 크거나, 자주 사용되지 않는 테이블의 경우는 지양합니다. 왜냐하면 데이터의 직렬화, 역직렬화에 따른 비용이 상당하기 때문에 오히려 전체적인 처리시간에 악영향을 줄 수 있습니다.\n",
    "* 메모리에 들어가지 않을 만큼 큰 데이터\n",
    "* 크기에 비해서 자주 사용되지 않는 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.3 A Family of Spark Joins\n",
    "\n",
    "```text\n",
    "/**\n",
    "   * Select the proper physical plan for join based on join strategy hints, the availability of\n",
    "   * equi-join keys and the sizes of joining relations. Below are the existing join strategies,\n",
    "   * their characteristics and their limitations.\n",
    "   *\n",
    "   * - Broadcast hash join (BHJ):\n",
    "   *     Only supported for equi-joins, while the join keys do not need to be sortable.\n",
    "   *     Supported for all join types except full outer joins.\n",
    "   *     BHJ usually performs faster than the other join algorithms when the broadcast side is\n",
    "   *     small. However, broadcasting tables is a network-intensive operation and it could cause\n",
    "   *     OOM or perform badly in some cases, especially when the build/broadcast side is big.\n",
    "   *\n",
    "   * - Shuffle hash join:\n",
    "   *     Only supported for equi-joins, while the join keys do not need to be sortable.\n",
    "   *     Supported for all join types.\n",
    "   *     Building hash map from table is a memory-intensive operation and it could cause OOM\n",
    "   *     when the build side is big.\n",
    "   *\n",
    "   * - Shuffle sort merge join (SMJ):\n",
    "   *     Only supported for equi-joins and the join keys have to be sortable.\n",
    "   *     Supported for all join types.\n",
    "   *\n",
    "   * - Broadcast nested loop join (BNLJ):\n",
    "   *     Supports both equi-joins and non-equi-joins.\n",
    "   *     Supports all the join types, but the implementation is optimized for:\n",
    "   *       1) broadcasting the left side in a right outer join;\n",
    "   *       2) broadcasting the right side in a left outer, left semi, left anti or existence join;\n",
    "   *       3) broadcasting either side in an inner-like join.\n",
    "   *     For other cases, we need to scan the data multiple times, which can be rather slow.\n",
    "   *\n",
    "   * - Shuffle-and-replicate nested loop join (a.k.a. cartesian product join):\n",
    "   *     Supports both equi-joins and non-equi-joins.\n",
    "   *     Supports only inner like joins.\n",
    "   */\n",
    "```\n",
    "### 7.3.1 Broadcast Hash Join\n",
    "> 드라이버 혹은 익스큐터의 메모리 보다 충분히 작은 경우에 해당 데이터를 broadcast 변수에 담아, 상대적으로 큰 데이터가 존재하는 노드로 변수를 전달하기 때문에 map 단계에서 join 이 일어나게 되어 *map-side-join* 이라고 부르며, 조인 성능에 가장 큰 영향을 미치는 셔플이 발생하지 않게 되어 성능이 좋습니다.\n",
    "\n",
    "#### When to use a broadcast hash join\n",
    "* 작고 큰 데이터 집합의 개별 키가 스파크에 의해서 같은 파티션에 해시되어 있는 경우\n",
    "  - 버킷 등을 통해 이미 동일한 노드에 저장되어 있는 경우로 추측\n",
    "  - hash-join 과 같이 hash table 을 사용하는 것처럼 보이지는 않으나 확인이 필요함\n",
    "* 하나의 데이터 집합이 다른 데이터 집합에 비해 훨씬 작을 때 (그리고 기본 구성 메모리가 충분한 경우 10MB 이상)\n",
    "* 정렬되지 않은 키들의 매칭을 기반으로 두 데이터집합 들을 결합하기 위해서, 동등 조인을 수행하기를 워한는 경우\n",
    "  - 해시 조인이기 때문에 정렬되지 않은 상태의 Equi-join 이 가능하기 때문\n",
    "  - [non equi-join](https://www.essentialsql.com/non-equi-join-sql-purpose/) 은 anti-join 혹은 range-join 이 있다\n",
    "* 모든 스파크 익스큐터들에 작은 데이터가 브로드캐스트 될 것이 명확해서, 네트워크 밴드나 OOM 오류를 걱정할 필요가 없을때\n",
    "\n",
    "#### spark.sql.autoBroadcastJoinThreshold 값으로 설정을 변경할 수 있으며, 기본 값은 10m 입니다\n",
    "![figure.7-6](images/figure.7-6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|name  |type|\n",
      "+------+----+\n",
      "|Cat   |1   |\n",
      "|Dog   |1   |\n",
      "|Monkey|2   |\n",
      "|Lion  |3   |\n",
      "|Tiger |3   |\n",
      "+------+----+\n",
      "\n",
      "+------+--------+\n",
      "|  name|category|\n",
      "+------+--------+\n",
      "|  Lion|   Beast|\n",
      "| Tiger|   Beast|\n",
      "|Monkey|  Animal|\n",
      "|   Cat|     Fat|\n",
      "|   Dog|     Fat|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "animal = spark.createDataFrame([(\"Cat\", 1), (\"Dog\", 1), (\"Monkey\", 2), (\"Lion\", 3), (\"Tiger\", 3)], [\"name\", \"type\"])\n",
    "animal.show(truncate=False)\n",
    "category = spark.createDataFrame([(\"Fat\", 1), (\"Animal\", 2), (\"Beast\", 3)], [\"category\", \"id\"])\n",
    "animal.join(category, animal.type == category.id, \"left_outer\").select(\"name\", \"category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|category|\n",
      "+------+--------+\n",
      "|   Cat|     Fat|\n",
      "|   Dog|     Fat|\n",
      "|Monkey|  Animal|\n",
      "|  Lion|   Beast|\n",
      "| Tiger|   Beast|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "animal.join(broadcast(category), animal.type == category.id, \"left_outer\").select(\"name\", \"category\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 스파크 3.0 에서 추가된 기능으로 explain 모드를 입력할 수 있으며 simple, extended, codegen, cost, formatted 등의 옵션을 제공합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "SortMergeJoin [type#330L], [id#343L], LeftOuter\n",
      ":- *(2) Sort [type#330L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(type#330L, 5), ENSURE_REQUIREMENTS, [id=#277]\n",
      ":     +- *(1) Scan ExistingRDD[name#329,type#330L]\n",
      "+- *(4) Sort [id#343L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(id#343L, 5), ENSURE_REQUIREMENTS, [id=#282]\n",
      "      +- *(3) Filter isnotnull(id#343L)\n",
      "         +- *(3) Scan ExistingRDD[category#342,id#343L]\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "* BroadcastHashJoin LeftOuter BuildRight (5)\n",
      ":- * Scan ExistingRDD (1)\n",
      "+- BroadcastExchange (4)\n",
      "   +- * Filter (3)\n",
      "      +- * Scan ExistingRDD (2)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD [codegen id : 2]\n",
      "Output [2]: [name#329, type#330L]\n",
      "Arguments: [name#329, type#330L], MapPartitionsRDD[68] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Scan ExistingRDD [codegen id : 1]\n",
      "Output [2]: [category#342, id#343L]\n",
      "Arguments: [category#342, id#343L], MapPartitionsRDD[75] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(3) Filter [codegen id : 1]\n",
      "Input [2]: [category#342, id#343L]\n",
      "Condition : isnotnull(id#343L)\n",
      "\n",
      "(4) BroadcastExchange\n",
      "Input [2]: [category#342, id#343L]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[1, bigint, false]),false), [id=#305]\n",
      "\n",
      "(5) BroadcastHashJoin [codegen id : 2]\n",
      "Left keys [1]: [type#330L]\n",
      "Right keys [1]: [id#343L]\n",
      "Join condition: None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "animal.join(category, animal.type == category.id, \"left_outer\").explain(\"simple\")\n",
    "animal.join(broadcast(category), animal.type == category.id, \"left_outer\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 소트머지조인 | 브로드캐스트 조인 |\n",
    "| --- | --- |\n",
    "| ![join_shuffle](images/join_shuffle.png) | ![join_broadcast](images/join_broadcast.png) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2 Shuffle Sort Merge Join\n",
    "> 두개의 대용량 데이터 집합을 조인하는 가장 효과적인 알고리즘이며, 기본 설정은 spark.sql.join.preferSortMergeJoin 은 enabled 된 상태입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreashold\", \"10485760b\") # default value\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreashold\", \"-1\") # force sortMergeJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  name|type|\n",
      "+------+----+\n",
      "|   Cat|   1|\n",
      "|   Dog|   1|\n",
      "|Monkey|   2|\n",
      "|  Lion|   3|\n",
      "| Tiger|   3|\n",
      "+------+----+\n",
      "\n",
      "+---+-----+\n",
      "| id| item|\n",
      "+---+-----+\n",
      "|  0|SKU-0|\n",
      "|  1|SKU-1|\n",
      "|  2|SKU-2|\n",
      "|  3|SKU-3|\n",
      "|  4|SKU-4|\n",
      "|  5|SKU-5|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "states = spark.createDataFrame([(0, \"AZ\"), (1, \"CO\"), (3, \"TX\"), (4, \"N\"), (5, \"MI\")], [\"id\", \"state\"])\n",
    "items = spark.createDataFrame([(0, \"SKU-0\"), (1, \"SKU-1\"), (2, \"SKU-2\"), (3, \"SKU-3\"), (4, \"SKU-4\"), (5, \"SKU-5\")], [\"id\", \"item\"])\n",
    "animal.show()\n",
    "items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+-------+-----+-----+---+-------+----------------------+----------+\n",
      "|transaction_id|quantity|users_id|amount |state|items|uid|login  |email                 |user_state|\n",
      "+--------------+--------+--------+-------+-----+-----+---+-------+----------------------+----------+\n",
      "|3582          |3582    |0       |7164.0 |MI   |SKU-0|0  |user_0 |user_0@databricks.com |CA        |\n",
      "|4135          |4135    |4       |8270.0 |CA   |SKU-5|4  |user_4 |user_4@databricks.com |CO        |\n",
      "|541           |541     |16      |1082.0 |AZ   |SKU-5|16 |user_16|user_16@databricks.com|CO        |\n",
      "|5919          |5919    |18      |11838.0|CA   |SKU-5|18 |user_18|user_18@databricks.com|CO        |\n",
      "|241           |241     |20      |482.0  |MI   |SKU-3|20 |user_20|user_20@databricks.com|CO        |\n",
      "|5188          |5188    |26      |10376.0|NY   |SKU-3|26 |user_26|user_26@databricks.com|MI        |\n",
      "|2971          |2971    |29      |5942.0 |CA   |SKU-4|29 |user_29|user_29@databricks.com|TX        |\n",
      "|5728          |5728    |29      |11456.0|MI   |SKU-4|29 |user_29|user_29@databricks.com|TX        |\n",
      "|6176          |6176    |29      |12352.0|TX   |SKU-0|29 |user_29|user_29@databricks.com|TX        |\n",
      "|5981          |5981    |32      |11962.0|CA   |SKU-4|32 |user_32|user_32@databricks.com|TX        |\n",
      "|744           |744     |36      |1488.0 |CA   |SKU-0|36 |user_36|user_36@databricks.com|CO        |\n",
      "|494           |494     |40      |988.0  |TX   |SKU-4|40 |user_40|user_40@databricks.com|MI        |\n",
      "|1207          |1207    |40      |2414.0 |AZ   |SKU-1|40 |user_40|user_40@databricks.com|MI        |\n",
      "|2537          |2537    |43      |5074.0 |AZ   |SKU-5|43 |user_43|user_43@databricks.com|CA        |\n",
      "|1423          |1423    |47      |2846.0 |CA   |SKU-1|47 |user_47|user_47@databricks.com|CO        |\n",
      "|8706          |8706    |53      |17412.0|TX   |SKU-0|53 |user_53|user_53@databricks.com|CO        |\n",
      "|9413          |9413    |53      |18826.0|MI   |SKU-0|53 |user_53|user_53@databricks.com|CO        |\n",
      "|2630          |2630    |61      |5260.0 |MI   |SKU-2|61 |user_61|user_61@databricks.com|CO        |\n",
      "|7284          |7284    |64      |14568.0|MI   |SKU-2|64 |user_64|user_64@databricks.com|AZ        |\n",
      "|1010          |1010    |68      |2020.0 |NY   |SKU-1|68 |user_68|user_68@databricks.com|TX        |\n",
      "+--------------+--------+--------+-------+-----+-----+---+-------+----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreashold\", \"-1\") # force sortMergeJoin\n",
    "\n",
    "states = {0:\"AZ\", 1:\"CO\", 2:\"CA\", 3: \"TX\", 4: \"NY\", 5:\"MI\"}\n",
    "items = {0:\"SKU-0\", 1:\"SKU-1\", 2:\"SKU-2\", 3: \"SKU-3\", 4: \"SKU-4\", 5:\"SKU-5\"}\n",
    "\n",
    "usersDF = spark.range(0, 10000).rdd.map(lambda id: (id[0], \"user_{}\".format(id[0]), \"user_{}@databricks.com\".format(id[0]), states[random.randint(0, 5)])).toDF([\"uid\", \"login\", \"email\", \"user_state\"])\n",
    "ordersDF = spark.range(0, 10000).rdd.map(lambda r: (r[0], r[0], random.randint(0, 10000), 10 * r[0] * 0.2, states[random.randint(0, 5)], items[random.randint(0,5)])).toDF([\"transaction_id\", \"quantity\", \"users_id\", \"amount\", \"state\", \"items\"])\n",
    "\n",
    "# usersDF.show(truncate=False)\n",
    "# ordersDF.show(truncate=False)\n",
    "\n",
    "usersOrdersDF = ordersDF.join(usersDF, ordersDF.users_id == usersDF.uid)\n",
    "usersOrdersDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* SortMergeJoin Inner (9)\n",
      ":- * Sort (4)\n",
      ":  +- Exchange (3)\n",
      ":     +- * Filter (2)\n",
      ":        +- * Scan ExistingRDD (1)\n",
      "+- * Sort (8)\n",
      "   +- Exchange (7)\n",
      "      +- * Filter (6)\n",
      "         +- * Scan ExistingRDD (5)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD [codegen id : 1]\n",
      "Output [6]: [transaction_id#438L, quantity#439L, users_id#440L, amount#441, state#442, items#443]\n",
      "Arguments: [transaction_id#438L, quantity#439L, users_id#440L, amount#441, state#442, items#443], MapPartitionsRDD[126] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [6]: [transaction_id#438L, quantity#439L, users_id#440L, amount#441, state#442, items#443]\n",
      "Condition : isnotnull(users_id#440L)\n",
      "\n",
      "(3) Exchange\n",
      "Input [6]: [transaction_id#438L, quantity#439L, users_id#440L, amount#441, state#442, items#443]\n",
      "Arguments: hashpartitioning(users_id#440L, 5), ENSURE_REQUIREMENTS, [id=#417]\n",
      "\n",
      "(4) Sort [codegen id : 2]\n",
      "Input [6]: [transaction_id#438L, quantity#439L, users_id#440L, amount#441, state#442, items#443]\n",
      "Arguments: [users_id#440L ASC NULLS FIRST], false, 0\n",
      "\n",
      "(5) Scan ExistingRDD [codegen id : 3]\n",
      "Output [4]: [uid#428L, login#429, email#430, user_state#431]\n",
      "Arguments: [uid#428L, login#429, email#430, user_state#431], MapPartitionsRDD[115] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(6) Filter [codegen id : 3]\n",
      "Input [4]: [uid#428L, login#429, email#430, user_state#431]\n",
      "Condition : isnotnull(uid#428L)\n",
      "\n",
      "(7) Exchange\n",
      "Input [4]: [uid#428L, login#429, email#430, user_state#431]\n",
      "Arguments: hashpartitioning(uid#428L, 5), ENSURE_REQUIREMENTS, [id=#423]\n",
      "\n",
      "(8) Sort [codegen id : 4]\n",
      "Input [4]: [uid#428L, login#429, email#430, user_state#431]\n",
      "Arguments: [uid#428L ASC NULLS FIRST], false, 0\n",
      "\n",
      "(9) SortMergeJoin [codegen id : 5]\n",
      "Left keys [1]: [users_id#440L]\n",
      "Right keys [1]: [uid#428L]\n",
      "Join condition: None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersOrdersDF.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing the shuffle sort merge join\n",
    "> Sort-Merge 조인의 가장 큰 비용인 Exchange Stage 를 제거하여 성능향상을 도모할 수 있습니다. 이는 버킷을 통해 해당 데이터를 생성하는 시점에 미리 정렬해 두는 접근입니다. 즉 자주 사용되는 equi-join 의 컬럼을 기준으로 버킷 수준에서 정렬해둔다고 보시면 됩니다.\n",
    "\n",
    "![join_exchange](images/join_exchange.png)\n",
    "![join_bucket](images/join_bucket.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "%rm -rf \"spark-warehouse/userstbl\"\n",
    "%rm -rf \"spark-warehouse/orderstbl\"\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "(\n",
    "    usersDF.orderBy(asc(\"uid\"))\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .bucketBy(8, \"uid\")\n",
    "    .saveAsTable(\"UsersTbl\")\n",
    ")\n",
    "\n",
    "(\n",
    "    ordersDF.orderBy(asc(\"users_id\"))\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .bucketBy(8, \"users_id\")\n",
    "    .saveAsTable(\"OrdersTbl\")\n",
    ")\n",
    "\n",
    "spark.sql(\"cache table UsersTbl\")\n",
    "spark.sql(\"cache table OrdersTbl\")\n",
    "\n",
    "usersBucketDF = spark.table(\"UsersTbl\")\n",
    "ordersBucketDF = spark.table(\"OrdersTbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+-------+-----+-----+---+--------+-----------------------+----------+\n",
      "|transaction_id|quantity|users_id|amount |state|items|uid|login   |email                  |user_state|\n",
      "+--------------+--------+--------+-------+-----+-----+---+--------+-----------------------+----------+\n",
      "|4774          |4774    |2       |9548.0 |MI   |SKU-2|2  |user_2  |user_2@databricks.com  |CO        |\n",
      "|4417          |4417    |12      |8834.0 |TX   |SKU-4|12 |user_12 |user_12@databricks.com |TX        |\n",
      "|3673          |3673    |26      |7346.0 |CA   |SKU-4|26 |user_26 |user_26@databricks.com |CA        |\n",
      "|149           |149     |28      |298.0  |AZ   |SKU-0|28 |user_28 |user_28@databricks.com |AZ        |\n",
      "|8754          |8754    |42      |17508.0|CO   |SKU-3|42 |user_42 |user_42@databricks.com |CO        |\n",
      "|4794          |4794    |48      |9588.0 |CO   |SKU-2|48 |user_48 |user_48@databricks.com |AZ        |\n",
      "|1556          |1556    |48      |3112.0 |TX   |SKU-5|48 |user_48 |user_48@databricks.com |AZ        |\n",
      "|3972          |3972    |67      |7944.0 |TX   |SKU-0|67 |user_67 |user_67@databricks.com |CO        |\n",
      "|8104          |8104    |73      |16208.0|AZ   |SKU-1|73 |user_73 |user_73@databricks.com |CA        |\n",
      "|5598          |5598    |88      |11196.0|AZ   |SKU-0|88 |user_88 |user_88@databricks.com |CA        |\n",
      "|6380          |6380    |91      |12760.0|AZ   |SKU-1|91 |user_91 |user_91@databricks.com |TX        |\n",
      "|4639          |4639    |91      |9278.0 |MI   |SKU-4|91 |user_91 |user_91@databricks.com |TX        |\n",
      "|8877          |8877    |93      |17754.0|MI   |SKU-1|93 |user_93 |user_93@databricks.com |CA        |\n",
      "|7928          |7928    |93      |15856.0|TX   |SKU-1|93 |user_93 |user_93@databricks.com |CA        |\n",
      "|7452          |7452    |93      |14904.0|CA   |SKU-3|93 |user_93 |user_93@databricks.com |CA        |\n",
      "|1153          |1153    |151     |2306.0 |MI   |SKU-5|151|user_151|user_151@databricks.com|MI        |\n",
      "|8380          |8380    |152     |16760.0|MI   |SKU-3|152|user_152|user_152@databricks.com|MI        |\n",
      "|5062          |5062    |152     |10124.0|CO   |SKU-5|152|user_152|user_152@databricks.com|MI        |\n",
      "|8813          |8813    |156     |17626.0|AZ   |SKU-4|156|user_156|user_156@databricks.com|AZ        |\n",
      "|3043          |3043    |156     |6086.0 |NY   |SKU-3|156|user_156|user_156@databricks.com|AZ        |\n",
      "+--------------+--------+--------+-------+-----+-----+---+--------+-----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinUsersOrdersBucketDF = ordersBucketDF.join(usersBucketDF, ordersBucketDF.users_id == usersBucketDF.uid)\n",
    "joinUsersOrdersBucketDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use a shuffle sort merge join\n",
    "* 두 개의 큰 데이터 집합의 각 키를 정렬하고 동일한 파티션으로 해시 할 수 있는 경우\n",
    "* 정렬된 키들의 매칭을 기반으로 두 데이터집합 들을 결합하기 위해서, 동등 조인을 수행하기를 워한는 경우\n",
    "  - SortMerge 조인이기 때문에 이미 정렬된 상태의 Equi-join 이 가능하기 때문\n",
    "* 네트워크를 통해 대용량 셔플 파일을 저장 시에 Exchange 와 Sort 연산을 피하고 싶을 때\n",
    "  - Bucket 기법을 활용하는 예제를 고려하라는 말로 추측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.4 Inspecting the Spark UI\n",
    "### 7.4.1 Journey Through the Spark UI Tabs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jobs and Stages\n",
    "> Duration 항목을 기준으로 문제가 되는 Job, Stage 및 Task 를 추측합니다\n",
    "\n",
    "* 확인 및 모니터링 대상 지표\n",
    "  - Average Duration 시간\n",
    "  - GC 에 소모되는 시간\n",
    "  - Shuffle bytes/records 정보\n",
    "\n",
    "![ch7-ui-1](images/ch7-ui-1.png)\n",
    "![ch7-ui-2](images/ch7-ui-2.png)\n",
    "\n",
    "![ch7-ui-3](images/ch7-ui-3.png)\n",
    "![ch7-ui-4](images/ch7-ui-4.png)\n",
    "![ch7-ui-5](images/ch7-ui-5.png)\n",
    "![ch7-ui-6](images/ch7-ui-6.png)\n",
    "![ch7-ui-7](images/ch7-ui-7.png)\n",
    "![ch7-ui-8](images/ch7-ui-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 추가로 학습할 내용들\n",
    "* [Tuning Apache Spark for Large Scale Workloads - Sital Kedia & Gaoxiang Liu](https://www.youtube.com/watch?v=5dga0UT4RI8)\n",
    "* [Hive Bucketing in Apache Spark - Tejas Patil](https://www.youtube.com/watch?v=6BD-Vv-ViBw)\n",
    "* [How does Facebook tune Apache Spark for Large-Scale Workloads?](https://towardsdatascience.com/how-does-facebook-tune-apache-spark-for-large-scale-workloads-3238ddda0830)\n",
    "* [External Shuffle Service in Apache Spark](https://www.waitingforcode.com/apache-spark/external-shuffle-service-apache-spark/read)\n",
    "* [Spark Internal Part 2. Spark의 메모리 관리(2)](https://medium.com/@leeyh0216/spark-internal-part-2-spark%EC%9D%98-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B4%80%EB%A6%AC-2-db1975b74d2f)\n",
    "* [Why You Should Care about Data Layout in the Filesystem](https://databricks.com/session/why-you-should-care-about-data-layout-in-the-filesystem)\n",
    "* [Five distinct join strategies](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala#L111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
