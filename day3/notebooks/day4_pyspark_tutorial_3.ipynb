{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4일차 3교시 데이터 타입\n",
    "\n",
    "### 목차\n",
    "* 1. 개요\n",
    "* 2. 스파크 데이터 타입으로 변환하기\n",
    "* 3. 불리언 데이터 타입 다루기\n",
    "* 4. 수치형 데이터 타입 다루기\n",
    "* 5. 문자열 데이터 타입 다루기\n",
    "* 6. 정규표현식\n",
    "* 7. 날짜와 타임스팸프 데이터 타입 다루기\n",
    "* 8. 널 값 다루기\n",
    "* 9. 복합 데이터 다루기\n",
    "* 10. JSON 다루기\n",
    "* 11. 사용자 정의 함수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. 개요\n",
    "+ DataFrame 메서드\n",
    "    + DataFrameStatFunctions : 다양한 통계적 함수를 제공\n",
    "    + DataFrameNaFunctions : null 데이터를 다루는데 필요한 함수를 제공\n",
    "+ Column 메서드\n",
    "    + alias, contains과 같은 컬럼과 관련된 여러가지 메서드를 제공\n",
    "    + org.apache.spark.sql.function 패키지는 데이터 타입과 관련된 다양한 함수를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1598173575671'),\n",
       " ('spark.driver.host', '7123e3456028'),\n",
       " ('spark.driver.port', '43995'),\n",
       " ('spark.app.name', 'Data Engineer Intermediate Day4'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.dataengineer.intermediate.day4', 'tutorial-3'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data Engineer Intermediate Day4\") \\\n",
    "    .config(\"spark.dataengineer.intermediate.day4\", \"tutorial-3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.getConf().getAll() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" DataFrame 생성 \"\"\"\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"retail\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 스파크 데이터 타입으로 변환하기\n",
    "+ lit 함수 : 상수 값을 리터럴 데이터 타입으로 변환합니다\n",
    "+ spark SQL에서는 스파크 데이터 타입으로 변경할 수 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 5.0: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 스파크 데이터 타입으로 변환 \"\"\"\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 불리언 데이터 타입 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|InvoiceNO|         Description|\n",
      "+---------+--------------------+\n",
      "|   536366|HAND WARMER UNION...|\n",
      "|   536366|HAND WARMER RED P...|\n",
      "|   536367|ASSORTED COLOUR B...|\n",
      "|   536367|POPPY'S PLAYHOUSE...|\n",
      "|   536367|POPPY'S PLAYHOUSE...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+--------------------+\n",
      "|InvoiceNO|         Description|\n",
      "+---------+--------------------+\n",
      "|   536366|HAND WARMER UNION...|\n",
      "|   536366|HAND WARMER RED P...|\n",
      "|   536367|ASSORTED COLOUR B...|\n",
      "|   536367|POPPY'S PLAYHOUSE...|\n",
      "|   536367|POPPY'S PLAYHOUSE...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 조건절 입력 \"\"\"\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "x1 = df.where(col(\"InvoiceNO\") != 536365).select(\"InvoiceNO\", \"Description\")\n",
    "x2 = df.where(\"InvoiceNO <> 536365\").select(\"InvoiceNO\", \"Description\")\n",
    "x3 = df.where(\"InvoiceNO = 536365\").select(\"InvoiceNO\", \"Description\")\n",
    "\n",
    "# 아래는 집합 연산을 통해 subtract 연산을 할 수 있습니다\n",
    "assert(0 == x1.subtract(x2).count())\n",
    "assert(0 == x2.subtract(x1).count())\n",
    "x1.show(5)\n",
    "x2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" an, or 사용한 불리언 표현식 \"\"\"\n",
    "\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1 # Locate the position of the first occurrence of substr column in the given string.\n",
    "                                                      # Returns null if either of the arguments are null.\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|StockCode|\n",
      "+---------+\n",
      "|      DOT|\n",
      "|     POST|\n",
      "|       C2|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|StockCode|\n",
      "+---------+\n",
      "|     POST|\n",
      "|        M|\n",
      "|      DOT|\n",
      "|        D|\n",
      "|       C2|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SparkSQL 을 이용한 is in 구문 사용\n",
    "from pyspark.sql.functions import desc\n",
    "df.select('StockCode').where(\"StockCode in ('DOT', 'POST', 'C2')\").distinct().show(5)\n",
    "df.select('StockCode').distinct().sort(desc('StockCode')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|added|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|    8|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|    8|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\"\"\" instr 함수 \"\"\"\n",
    "df.withColumn(\"added\", instr(df.Description, \"POSTAGE\")).where(\"added > 1\").show() # 8번째 글자에 'POSTAGE'가 시작됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|unitPrice|isExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 불리언 컬럼 \"\"\"\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") > 1 # 'POSTAGE' 글자가 첫번째가 아닌 경우\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter)) \\\n",
    "    .where(\"isExpensive\") \\\n",
    "    .select(\"unitPrice\", \"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|   Description|UnitPrice|\n",
      "+--------------+---------+\n",
      "|DOTCOM POSTAGE|   569.77|\n",
      "|DOTCOM POSTAGE|   607.49|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" SQL문 실행 \"\"\"\n",
    "from pyspark.sql.functions import expr, col # 파이썬은 not이 존재하지 않는다. \n",
    "\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\")) \\\n",
    "    .where(\"isExpensive\") \\\n",
    "    .select(\"Description\", \"UnitPrice\").show(5)\n",
    "# SQL을 사용해도 성능은 차이나지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 수치형 데이터 타입 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|             489.0|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 지수만큼 제곱하는 pow 함수 \"\"\"\n",
    "from pyspark.sql.functions import expr, pow\n",
    "\n",
    "# 아래의 연산이 필요한 경우에는 반드시 column 으로 지정되어야 연산자 계산이 됩니다. (문자열 * 연산자는 없습니다)\n",
    "fabricateQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerID\"), fabricateQuantity.alias(\"realQuantity\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|             489.0|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 구조화 API 가 어렵다면 Expression 을 이용하여 표현해도 결과는 동일합니다.\n",
    "fabricateQuantity = expr(\"pow(Quantity * UnitPrice, 2) + 5\")\n",
    "df.select(expr(\"CustomerID\"), fabricateQuantity.alias(\"realQuantity\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 1)|bround(2.5, 1)|\n",
      "+-------------+--------------+\n",
      "|          2.5|           2.5|\n",
      "|          2.5|           2.5|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 반올림하는 round 함수 \"\"\"\n",
    "from pyspark.sql.functions import lit, round, bround # round(반올림), bround(반내림)\n",
    "\n",
    "df.select(round(lit(\"2.5\"), 1), bround(lit(\"2.5\"), 1)).show(2) # 1차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04112314436835551"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 피어슨 상관계수를 계산 \"\"\"\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "df.stat.corr(\"Quantity\", \"UnitPrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|        InvoiceNo|\n",
      "+-------+-----------------+\n",
      "|  count|             3108|\n",
      "|   mean| 536516.684944841|\n",
      "| stddev|72.89447869788873|\n",
      "|    min|           536365|\n",
      "|    max|          C536548|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 요약 통계를 계산 \"\"\"\n",
    "df.describe().show()\n",
    "df.describe(\"InvoiceNo\").show() # 컬럼을 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ StaFunctions 패키지의 통계 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 백분위수를 구하는 approxQuantile \"\"\"\n",
    "olName = \"UnitPrice\"\n",
    "quantileProbs = [0.5]\n",
    "relError = 0.05\n",
    "\n",
    "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)\n",
    "# :relError: The relative target precision to achieve\n",
    "#   (>= 0). If set to zero, the exact quantiles are computed, which\n",
    "#   could be very expensive. Note that values greater than 1 are\n",
    "#   accepted but give the same result as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|StockCode|\n",
      "+---------+\n",
      "|    22728|\n",
      "|    21889|\n",
      "|   90210B|\n",
      "|    21259|\n",
      "|    21894|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+\n",
      "|Quantity|\n",
      "+--------+\n",
      "|      34|\n",
      "|      -1|\n",
      "|      28|\n",
      "|      27|\n",
      "|     384|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21080|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
      "|             22219|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 교차표를 생성하는 crosstab \"\"\"\n",
    "df.select(\"StockCode\").distinct().show(5) # rows name\n",
    "df.select(\"Quantity\").distinct().show(5)  # columns name\n",
    "df.stat.crosstab(\"StockCode\", \"Quantity\").show(5) # pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "|                            2|\n",
      "|                            3|\n",
      "|                            4|\n",
      "+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 고유 아이디를 생성하는 monotonically_increasing_id \"\"\"\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df.select(monotonically_increasing_id()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 문자열 데이터 타입 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|initcap(Description)              |\n",
      "+----------------------------------+\n",
      "|White Hanging Heart T-light Holder|\n",
      "|White Metal Lantern               |\n",
      "+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경, initcap \"\"\" \n",
    "from pyspark.sql.functions import initcap\n",
    "\n",
    "df.select(initcap(col(\"Description\"))).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|  lower(Description)|  upper(Description)|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 소문자/대문자 변경, lower/upper \"\"\"\n",
    "from pyspark.sql.functions import lower, upper\n",
    "\n",
    "df.select(col(\"Description\"), lower(col(\"Description\")), upper(col(\"Description\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+---+----------+\n",
      "|   ltrim|   rtrim| trim| lp|        rp|\n",
      "+--------+--------+-----+---+----------+\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLO     |\n",
      "+--------+--------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 문자열 주변의 공백을 제거, lpad/ltrim/rpad/rtrim/trim \"\"\"\n",
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "\n",
    "df.select(\n",
    "    ltrim(lit(\"   HELLO   \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"   HELLO   \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"   HELLO   \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 정규 표현식\n",
    "+ 존재 여부를 확인하거나 일치하는 모든 문자열을 치환\n",
    "+ 정규 표현식을 위해 regexp_extract 함수와 regexp_replace 함수를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         color_clean|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 단어 치환, regexp_extract \"\"\"\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "regex_string = \"BLACK|WHITE|RED|GRENN|BLUE\"\n",
    "df.select(\n",
    "    regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n",
    "    col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         Description|          Translated|\n",
      "+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|WHI2 HANGING H2AR...|\n",
      "| WHITE METAL LANTERN|    WHI2 M2A1 1AN2RN|\n",
      "|CREAM CUPID HEART...|CR2AM CUPID H2ARS...|\n",
      "|KNITTED UNION FLA...|KNI2D UNION F1AG ...|\n",
      "|RED WOOLLY HOTTIE...|R2D WOO11Y HOI2 W...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 문자 치환, translate \"\"\"\n",
    "from pyspark.sql.functions import translate\n",
    "df.select(\n",
    "    col(\"Description\"),\n",
    "    translate(col(\"Description\"), \"LEET\", \"12\").alias(\"Translated\") # 정확히 매칭되지 않아도 부분만 적용됩니다 L:1, E:2\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         Description|Extracted|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|    WHITE|\n",
      "| WHITE METAL LANTERN|    WHITE|\n",
      "+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 단어 추출, regexp_extract \"\"\"\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "extract_str = \"(BLACK|WHITE|RED|GRENN|BLUE)\"\n",
    "df.select(\n",
    "    col(\"Description\"),\n",
    "    regexp_extract(col(\"Description\"), extract_str, 1).alias(\"Extracted\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "|WOOD 2 DRAWER CABINET WHITE FINISH|\n",
      "|WOOD S/3 CABINET ANT WHITE FINISH |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 단어 존재유무, contain \"\"\" # 파이썬과 SQL은 instr 함수를 사용\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "containBlack = instr(col(\"Description\"), \"BLACK\") > 1\n",
    "containWhite = instr(col(\"Description\"), \"WHITE\") > 1\n",
    "df.withColumn(\"hasSimpleColor\", containBlack | containWhite) \\\n",
    "    .where(\"hasSimpleColor\") \\\n",
    "    .select(\"Description\") \\\n",
    "    .show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|is_black|is_white|is_red|is_green|is_blue|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+--------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   false|    true| false|   false|  false|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   false|    true| false|   false|  false|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   false|   false| false|   false|  false|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "+--------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 필드에 색깔 문자열이 포함되어 있는지 여부를 locate 함수를 이용하여 컬럼으로 생성하는 에제 \"\"\"\n",
    "from pyspark.sql.functions import expr, locate \n",
    "\n",
    "simple_colors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "\n",
    "def color_locator(column, color_string):   # color_strings 단어가 시작되는 문자기준(단어기준 X) 위치\n",
    "    return locate(color_string.upper(), column).cast(\"boolean\").alias(\"is_\" + color_string)\n",
    "\n",
    "selected_cols = [color_locator(df.Description, c) for c in simple_colors] # locate 함수를 하나씩 list에 저장\n",
    "selected_cols.append(expr(\"*\")) # column 타입이여야 함 \n",
    "\n",
    "df.select(*selected_cols).show(3)\n",
    "\n",
    "df.select(*selected_cols).where(expr(\"is_white OR is_red\")) \\\n",
    "    .select(col(\"Description\")) \\\n",
    "    .show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'CAST(locate(BLACK, Description, 1) AS BOOLEAN) AS `is_black`'>,\n",
       " Column<b'CAST(locate(WHITE, Description, 1) AS BOOLEAN) AS `is_white`'>,\n",
       " Column<b'CAST(locate(RED, Description, 1) AS BOOLEAN) AS `is_red`'>,\n",
       " Column<b'CAST(locate(GREEN, Description, 1) AS BOOLEAN) AS `is_green`'>,\n",
       " Column<b'CAST(locate(BLUE, Description, 1) AS BOOLEAN) AS `is_blue`'>,\n",
       " Column<b'unresolvedstar()'>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 날짜와 타임스팸프 데이터 타입 다루기\n",
    "+ 스파크는 2가지 시간 정보만 다룸\n",
    "    + 날짜 정보만 가지는 date\n",
    "    + 날짜와 시간 정보를 모두 가지는 timestamp\n",
    "+ 시간대 설정이 필요하다면 스파크 SQL 설정의 spark.conf.sessionLocalTimeZone 속성으로 가능\n",
    "    + 자바 TimeZone 포맷을 따라야 함\n",
    "+ TimestampType 클래스는 초 단위 정밀도만 지원\n",
    "    + 초 단위 이상 정밀도 요구 시 long 데이터 타입으로 데이터를 변환해 처리하는 우회 정책이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n",
      "+---+----------+-----------------------+\n",
      "|id |today     |now                    |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2020-08-23|2020-08-23 09:44:23.097|\n",
      "|1  |2020-08-23|2020-08-23 09:44:23.097|\n",
      "|2  |2020-08-23|2020-08-23 09:44:23.097|\n",
      "+---+----------+-----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 오늘 날짜 구하기 \"\"\"\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = spark.range(10) \\\n",
    "    .withColumn(\"today\", current_date()) \\\n",
    "    .withColumn(\"now\", current_timestamp())\n",
    "\n",
    "dateDF.createOrReplaceTempView(\"dataTable\")\n",
    "dateDF.printSchema()\n",
    "\n",
    "dateDF.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2020-08-18|        2020-08-28|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 날짜를 더하거나 빼기 \"\"\"\n",
    "from pyspark.sql.functions import date_sub, date_add\n",
    "\n",
    "dateDF.select(\n",
    "    date_sub(col(\"today\"), 5),\n",
    "    date_add(col(\"today\"), 5)\n",
    ").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 두 날짜 사이의 일/개월 수를 파악 \"\"\"\n",
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)) \\\n",
    "    .select(datediff(col(\"week_ago\"), col(\"today\"))) \\\n",
    "    .show(1) # 현재 날짜에서 7일 제외 후 datediff 결과 확인\n",
    "\n",
    "dateDF \\\n",
    "    .select(to_date(lit(\"2016-01-01\")).alias(\"start\"), to_date(lit(\"2017-05-22\")).alias(\"end\")) \\\n",
    "    .select(months_between(col(\"start\"), col(\"end\"))).show(1) # 개월 수 차이 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2017-01-01|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 문자열을 날짜로 변환 \"\"\" # 자바의 simpleDateFormat 클래스가 지원하는 포맷 사용 필요\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "spark.range(5) \\\n",
    "    .withColumn(\"date\", lit(\"2017-01-01\")) \\\n",
    "    .select(to_date(col(\"date\"))) \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 파싱오류로 날짜가 null로 반환되는 사례 \"\"\"\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")), to_date(lit(\"2017-12-11\"))).show(1) # 월과 일의 순서가 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SimpleDateFormat 표준을 활용하여 날짜 포멧을 지정 \"\"\"\n",
    "from pyspark.sql.functions import to_date\n",
    "dateFormat = \"yyyy-dd-MM\" # 소문자 mm 주의\n",
    "cleanDateDF = spark.range(1).select( # 1개 Row를 생성\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ SimpleDateFormat : https://bvc12.tistory.com/168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * from dateTable2\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|to_timestamp(`date`, 'yyyy-dd-MM')|\n",
      "+----------------------------------+\n",
      "|               2017-11-12 00:00:00|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 항상 날짜 포맷을 지정해야 하는 to_timestamp 함수 \"\"\"\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 날짜 비교 \"\"\"\n",
    "cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 널 값 다루기\n",
    "+ null 값을 사용하는 것 보다 명시적으로 사용하는 것이 항상 좋음\n",
    "+ null 값을 허용하지 않는 컬럼을 선언해도 강제성은 없음\n",
    "+ nullable 속성은 스파크 SQL 옵티마이저가 해당 컬럼을 제어하는 동작을 단순하게 돕는 역할\n",
    "+ null 값을 다루는 방법은 두 가지 \n",
    "    + 명시적으로 null을 제거\n",
    "    + 전역 또느 컬럼 단위로 null 값을 특정 값으로 채움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-1. coalesce w/ null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|coalesce(Description, CustomerId)|\n",
      "+---------------------------------+\n",
      "|             WHITE HANGING HEA...|\n",
      "|              WHITE METAL LANTERN|\n",
      "|             CREAM CUPID HEART...|\n",
      "|             KNITTED UNION FLA...|\n",
      "|             RED WOOLLY HOTTIE...|\n",
      "|             SET 7 BABUSHKA NE...|\n",
      "|             GLASS STAR FROSTE...|\n",
      "|             HAND WARMER UNION...|\n",
      "|             HAND WARMER RED P...|\n",
      "|             ASSORTED COLOUR B...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             FELTCRAFT PRINCES...|\n",
      "|             IVORY KNITTED MUG...|\n",
      "|             BOX OF 6 ASSORTED...|\n",
      "|             BOX OF VINTAGE JI...|\n",
      "|             BOX OF VINTAGE AL...|\n",
      "|             HOME BUILDING BLO...|\n",
      "|             LOVE BUILDING BLO...|\n",
      "|             RECIPE BOX WITH M...|\n",
      "+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# coalesce : 인수로 지정한 여러 컬럼 중 null이 아닌 첫번 째 값 반환 하는 함수\n",
    "from pyspark.sql.functions import coalesce \n",
    "\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-2. ifnull, nullIf, nvl, nvl2\n",
    "+ SQL 함수이며 DataFrame의 select 표현식으로 사용 가능\n",
    "    + ifnull(null, 'return_value') # 두 번째 값을, 아니라면 첫 번째 값을 반환 \n",
    "    + nullif('value', 'value')     # 두 값이 같으면 null\n",
    "    + nvl(null, 'return_value')    # 두 번째 값을, 아니라면 첫 번째 값을 반환\n",
    "    + nvl2('not_null', 'return_value', 'else_value') # 두 번째 값을, 아니라면 세번째 값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "|ifnull(NULL, 'return_value')|nullif('value', 'value')|nvl(NULL, 'return_value')|nvl2('not null', 'return_value', 'else_value')|\n",
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "|                return_value|                    null|             return_value|                                  return_value|\n",
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    ifnull(null, 'return_value'),\n",
    "    nullif('value', 'value'),\n",
    "    nvl(null, 'return_value'),\n",
    "    nvl2('not null', 'return_value', 'else_value')\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-3 drop\n",
    "+ null 값을 가진 로우를 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop()\n",
    "df.na.drop(\"any\").show(1) # 로우 컬럼값 중 하나라도 null이면 제거\n",
    "df.na.drop(\"all\").show(1) # 로우 컬럼값 모두 null이면 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 배열 형태의 컬럼을 인수로 전달하여 지정한 컬럼만 제거합니다\n",
    "df.na.drop(\"all\", subset=(\"StockCode\", \"InvoiceNo\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 fill\n",
    "+ fill: null을 특정한 값으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|        null|        5.0|\n",
      "|       null|       World|       null|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" null을 포함한 DataFrame 행성 \"\"\"\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"string_null\", StringType(), True),\n",
    "    StructField(\"string2_null\", StringType(), True),\n",
    "    StructField(\"number_null\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "myRows = []\n",
    "myRows.append(Row(\"Hello\", None, float(5))) # string 컬럼에 null 포함\n",
    "myRows.append(Row(None, \"World\", None))     # number 컬럼에 null 포함\n",
    "\n",
    "myDf = spark.createDataFrame(myRows, myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ fill 함수는 DataType이 동일한 컬럼의 null만 치완\n",
    "+ 숫자형 또한 치환할 값의 DataType이 동일해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+\n",
      "|         string_null|        string2_null|number_null|\n",
      "+--------------------+--------------------+-----------+\n",
      "|               Hello|All null valus be...|        5.0|\n",
      "|All null valus be...|               World|       null|\n",
      "+--------------------+--------------------+-----------+\n",
      "\n",
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|        null|        5.0|\n",
      "|       null|       World|        5.0|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.na.fill(\"All null valus become this string\").show()\n",
    "myDf.na.fill(5.0).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|        null|        5.0|\n",
      "|   No Value|       World|        5.0|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 딕셔너리 타입을 사용해서 다수의 컬럼에 fill 메서드를 적용 \"\"\"\n",
    "fill_cols_vals = {\"number_null\": 5.0, \"string_null\": \"No Value\"}\n",
    "myDf.na.fill(fill_cols_vals).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-5 replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|        null|        5.0|\n",
      "|       null|       World|       null|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 조건에 따라 다른 값으로 대체 \"\"\"\n",
    "myDf.na.replace([\"\"], [\"Hello\"], \"string_null\").show() # null을 지정하는 방법은?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-6 정렬하기\n",
    "> asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 복합 데이터 다루기\n",
    "+ 구조체, 배열, 맵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9-1 구조체\n",
    "+ DataFrame 내부의 DataFrame\n",
    "+ 다수의 컬럼을 괄호로 묶어 생성 가능\n",
    "+ 문법에 점(.)을 사용하거나 getField 메서드를 사용\n",
    "+ (*) 문자로 모든 값을 조회할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|complex                                      |\n",
      "+---------------------------------------------+\n",
      "|[WHITE HANGING HEART T-LIGHT HOLDER, 536365] |\n",
      "|[WHITE METAL LANTERN, 536365]                |\n",
      "|[CREAM CUPID HEARTS COAT HANGER, 536365]     |\n",
      "|[KNITTED UNION FLAG HOT WATER BOTTLE, 536365]|\n",
      "|[RED WOOLLY HOTTIE WHITE HEART., 536365]     |\n",
      "+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- complex: struct (nullable = false)\n",
      " |    |-- Description: string (nullable = true)\n",
      " |    |-- InvoiceNo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "complexDF.show(5, False)\n",
    "complexDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         Description|InvoiceNo|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|   536365|\n",
      "| WHITE METAL LANTERN|   536365|\n",
      "|CREAM CUPID HEART...|   536365|\n",
      "|KNITTED UNION FLA...|   536365|\n",
      "|RED WOOLLY HOTTIE...|   536365|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.select(\"complex.Description\", \"complex.InvoiceNo\") # 모두 동일\n",
    "complexDF.select(col(\"complex\").getField(\"Description\"), col(\"complex\").getField(\"InvoiceNo\"))\n",
    "complexDF.select(\"complex.*\")\n",
    "complexDF.select(col(\"complex.*\"))\n",
    "complexDF.selectExpr(\"complex.*\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9-2 배열\n",
    "+ 데이터에서 Description 컬럼의 모든 단어를 하나의 로우로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|split(Description,  )|\n",
      "+---------------------+\n",
      "| [WHITE, HANGING, ...|\n",
      "| [WHITE, METAL, LA...|\n",
      "+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 컬럼을 배열로 변환 \"\"\"\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 배열값의 조회 \"\"\"\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n",
    "    .selectExpr(\"array_col[0]\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 배열의 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|size(split(Description,  ))|\n",
      "+---------------------------+\n",
      "|                          5|\n",
      "|                          3|\n",
      "+---------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" size 함수 \"\"\"\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### array_contains\n",
    "+ array_contains 함수를 사용해 배열에 특정 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|array_contains(split(Description,  ), WHITE)|\n",
      "+--------------------------------------------+\n",
      "|                                        true|\n",
      "|                                        true|\n",
      "+--------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explode\n",
    "+ 배열 타입의 컬럼을 입력받고 컬럼의 배열값에 포함된 모든 값을 로우로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- splitted: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- exploded: string (nullable = true)\n",
      "\n",
      "3108\n",
      "14414\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "exploded = df \\\n",
    "    .withColumn(\"splitted\", split(col(\"Description\"), \" \")) \\\n",
    "    .withColumn(\"exploded\", explode(col(\"splitted\")))\n",
    "exploded.printSchema()\n",
    "\n",
    "ef = exploded.select(\"Description\", \"InvoiceNo\", \"exploded\") # 모든 단어가 하나의 로우로 전환됨\n",
    "print(df.select(\"Description\").count())\n",
    "print(ef.select(\"exploded\").count()) # 로우 수가 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14414"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explodedDf.select(\"Description\", \"exploded\").count() # 큰 쪽으로 카운드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='WHITE'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='HANGING'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='HEART'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='T-LIGHT'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='HOLDER'),\n",
       " Row(Description='WHITE METAL LANTERN', exploded='WHITE'),\n",
       " Row(Description='WHITE METAL LANTERN', exploded='METAL'),\n",
       " Row(Description='WHITE METAL LANTERN', exploded='LANTERN'),\n",
       " Row(Description='CREAM CUPID HEARTS COAT HANGER', exploded='CREAM'),\n",
       " Row(Description='CREAM CUPID HEARTS COAT HANGER', exploded='CUPID')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explodedDf.select(\"Description\", \"exploded\").take(10) # Description 컬럼이 Group이 되어 중복됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9-3 맵\n",
    "+ map 함수와 컬럼의 키0값 쌍을 이용해 생성\n",
    "+ 적합한 키를 사용해 데이터를 조회할 수 있으며, 해당키가 없다면 null값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|complex_map                                    |\n",
      "+-----------------------------------------------+\n",
      "|[WHITE HANGING HEART T-LIGHT HOLDER -> 536365] |\n",
      "|[WHITE METAL LANTERN -> 536365]                |\n",
      "|[CREAM CUPID HEARTS COAT HANGER -> 536365]     |\n",
      "|[KNITTED UNION FLAG HOT WATER BOTTLE -> 536365]|\n",
      "|[RED WOOLLY HOTTIE WHITE HEART. -> 536365]     |\n",
      "|[SET 7 BABUSHKA NESTING BOXES -> 536365]       |\n",
      "|[GLASS STAR FROSTED T-LIGHT HOLDER -> 536365]  |\n",
      "|[HAND WARMER UNION JACK -> 536366]             |\n",
      "|[HAND WARMER RED POLKA DOT -> 536366]          |\n",
      "|[ASSORTED COLOUR BIRD ORNAMENT -> 536367]      |\n",
      "|[POPPY'S PLAYHOUSE BEDROOM  -> 536367]         |\n",
      "|[POPPY'S PLAYHOUSE KITCHEN -> 536367]          |\n",
      "|[FELTCRAFT PRINCESS CHARLOTTE DOLL -> 536367]  |\n",
      "|[IVORY KNITTED MUG COSY  -> 536367]            |\n",
      "|[BOX OF 6 ASSORTED COLOUR TEASPOONS -> 536367] |\n",
      "|[BOX OF VINTAGE JIGSAW BLOCKS  -> 536367]      |\n",
      "|[BOX OF VINTAGE ALPHABET BLOCKS -> 536367]     |\n",
      "|[HOME BUILDING BLOCK WORD -> 536367]           |\n",
      "|[LOVE BUILDING BLOCK WORD -> 536367]           |\n",
      "|[RECIPE BOX WITH METAL HEART -> 536367]        |\n",
      "+-----------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 맵 생성 \"\"\"\n",
    "from pyspark.sql.functions import create_map\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- complex_map: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                          536365|\n",
      "|                          536373|\n",
      "|                          536375|\n",
      "|                          536396|\n",
      "|                          536406|\n",
      "|                          536544|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 맵의 데이터 조회 \"\"\"\n",
    "mapped = df \\\n",
    "    .select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n",
    "mapped.printSchema()\n",
    "mapped.selectExpr(\"complex_map['WHITE METAL LANTERN']\").where(\"complex_map['WHITE METAL LANTERN'] is not null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = false)\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+------+\n",
      "|                 key| value|\n",
      "+--------------------+------+\n",
      "|WHITE HANGING HEA...|536365|\n",
      "| WHITE METAL LANTERN|536365|\n",
      "|CREAM CUPID HEART...|536365|\n",
      "|KNITTED UNION FLA...|536365|\n",
      "|RED WOOLLY HOTTIE...|536365|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 맵의 분해 \"\"\"\n",
    "exploded = df \\\n",
    "    .select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")) \\\n",
    "    .selectExpr(\"explode(complex_map)\")\n",
    "exploded.printSchema()\n",
    "exploded.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. JSON 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Json 컬럼 생성 \"\"\"\n",
    "jsonDF = spark.range(1).selectExpr(\n",
    "    \"\"\"\n",
    "    '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|column|                  c0|\n",
      "+------+--------------------+\n",
      "|  null|{\"myJSONValue\":[1...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 인라인 쿼리로 JSON 조회하기 \"\"\"\n",
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"jsonString.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(structstojson(myStruct)='{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}'),\n",
       " Row(structstojson(myStruct)='{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}'),\n",
       " Row(structstojson(myStruct)='{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" StructType을 Json 문자열로 변경 \"\"\"\n",
    "from pyspark.sql.functions import to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\") \\\n",
    "    .select(to_json(col(\"myStruct\"))) \\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------+\n",
      "|jsontostructs(newJSON)|             newJSON|\n",
      "+----------------------+--------------------+\n",
      "|  [536365, WHITE HA...|{\"InvoiceNo\":\"536...|\n",
      "|  [536365, WHITE ME...|{\"InvoiceNo\":\"536...|\n",
      "+----------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Json 문자열을 객체로 변환 \"\"\"\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "parseSchema = StructType([\n",
    "    StructField(\"InvoiceNo\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True)\n",
    "])\n",
    "\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\") \\\n",
    "    .select(to_json(col(\"myStruct\")).alias(\"newJSON\")) \\\n",
    "    .select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")) \\\n",
    "    .show(2) # 키를 컬럼명으로 값을 로우로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 사용자 정의 함수 \n",
    "+ User defined function(UDF)는 레포트별로 데이터를 처리하는 함수이며, SparkSession이나 Context에서 사용할 수 있도록 임시 함수 형태로 등록됨\n",
    "+ 내장 함수가 제공하는 코드 생성 기능의 장점을 활용할 수 없어 약간의 성능 저하 발생\n",
    "+ 언어별로 성능차이가 존재, 파이썬에서도 사용할 수 있으므로 자바나 스칼라도 함수 작성을 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" UDF 사용하기 \"\"\"\n",
    "udfExDF = spark.range(5).toDF(\"num\")\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" UDF 등록 및 사용 \"\"\"\n",
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)\n",
    "\n",
    "udfExDF.select(power3udf(col(\"num\"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스칼라에서 등록되 사용자 정의 함수를 파이썬에서 활용:\n",
    "\n",
    "https://www.cyanny.com/2017/09/15/spark-use-scala-udf-udaf-in-pyspark/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
