{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크 스트리밍 실습 3교시 : 스트리밍 모니터링 및 조회\n",
    "\n",
    "> 스트리밍 애플리케이션을 모니터링 하는 기법을 익히고, 메모리 테이블을 생성하고, 노트북 상에서 활용할 수 있는 도구들을 통해서 좀 더 간편하게 디버깅 및 테스트 할 수 있습니다.\n",
    "\n",
    "## 학습 목표\n",
    "* 소켓 스트리밍 애플리케이션을 실행하고, 스트리밍 쿼리 모니터링을 실습합니다\n",
    "  - `StreamingQueryProgress` 객체를 통한 모니터링 실습\n",
    "  - `Dropwizard Metrics` 통한 Web UI 통한 모니터링 실습\n",
    "* 메모리 테이블 싱크를 통해 테이블을 생성하고 Spark SQL 통한 조회를 실습합니다\n",
    "  - JSON 파일을 소스로 하는 데이터 소스를 생성합니다\n",
    "  - 집계결과를 메모리 싱크 테이블로 출력합니다\n",
    "  - 셀 출력화면에 메모리 싱크 테이블 조회 결과를 출력하는 함수를 작성합니다\n",
    "  - 결과 테이블을 `Spark SQL` 통하여 조회 합니다\n",
    "\n",
    "## 목차\n",
    "* [1. 스트리밍 쿼리 상태를 통한 모니터링 실습](#1.-스트리밍-쿼리-상태를-통한-모니터링-실습)\n",
    "* [2. 메모리 싱크 테이블을 통한 조회 실습](#2.-메모리-싱크-테이블을-통한-조회-실습)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/28 15:34:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://383e5fd7e261:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f673f5fedc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "\n",
    "# 현재 기동된 스파크 애플리케이션의 포트를 확인하기 위해 스파크 정보를 출력합니다\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. 스트리밍 쿼리 상태를 통한 모니터링 실습\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 StreamingQuery 통하여 실행중인 쿼리 모니터링 하기\n",
    "\n",
    "> '구조화된 스트리밍' 특성상, 실행중인 쿼리를 모니터링하는 기능은 아주 중요하며 다양한 방법을 통해 모니터링 할 수 있습니다\n",
    "\n",
    "* 2교시에서 수행한 소켓 스트리밍 서버를 통한 예제 애플리케이션을 활용합니다\n",
    "  - 아래의 명령으로 소켓 서버를 기동하고 스트리밍 애플리케이션을 기동합니다\n",
    "```bash\n",
    "# terminal\n",
    "nc -lvp 9999 -n\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/28 15:34:21 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "# 소켓 서버로부터 단어를 가져와서 출력하는 가장 간단한 예제를 통해 모니터링 합니다\n",
    "\n",
    "# step1: 데이터를 읽어올 스트림 리더를 생성합니다\n",
    "wordCountHost = \"localhost\"\n",
    "wordCountPort = 9999\n",
    "wordCountReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"socket\")\n",
    "    .option(\"host\", wordCountHost)\n",
    "    .option(\"port\", wordCountPort)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# step2: 데이터로부터 단어의 수를 세는 카운터를 생성합니다\n",
    "wordCounter = wordCountReader.select(explode(split(col(\"value\"), \"\\s\")).alias(\"word\")).groupBy(\"Word\").count().alias(\"Count\")\n",
    "\n",
    "# step3: 생성된 수치를 콘솔에 출력하는 출력을 생성합니다\n",
    "queryName = \"wordCount\"\n",
    "wordCountWriter = (\n",
    "    wordCounter\n",
    "    .writeStream\n",
    "    .queryName(queryName) # 쿼리 테이블의 이름을 지정합니다\n",
    "    .format(\"console\") # 결과를 콘솔에 출력합니다\n",
    "    .outputMode(\"complete\") # 매번 전체 데이터를 내보냅니다\n",
    ")\n",
    "\n",
    "# step4: 얼마나 자주 수행될 지를 결정하는 트리거를 생성합니다\n",
    "wordCountCheckpointDir = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $wordCountCheckpointDir # 경우에 따라서 이미 존재하는 경로의 경우 오류가 발생할 수 있기 때문에 항상 제거합니다\n",
    "wordCountTrigger = (\n",
    "    wordCountWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", wordCountCheckpointDir)\n",
    ")\n",
    "\n",
    "# step5: 해당 \n",
    "wordCountQuery = wordCountTrigger.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> lastProgress 객체는 마지막의 상태를 저장하고 있으므로 마지막으로 수행한 애플리케이션의 상태를 확인하기 위한 용도로 사용됩니다.\n",
    "\n",
    "* lastProgress 통해 StreamingQueryProgress(dictionary) 객체를 통해 확인합니다\n",
    "\n",
    "| 컬럼 | 설명 |\n",
    "| --- | --- |\n",
    "| id | Unique Identifier - 체크포인트 위치와 1:1 매칭되는 유일한 구분자로, 체크포인트 경로가 삭제되기 전까지는 동일한 값이 유지됩니다 |\n",
    "| runId | Unique Identifier - 현재 (지)시작된 쿼리 인스턴스를 가리키는 구분자이며, 매 실행시마다 변경됩니다 |\n",
    "| numInputRows | 마지막 '마이크로 배치' 작업에 수행 했던 입력 로우의 수 |\n",
    "| inputRowsPerSecond | 데이터 소스로부터 입력 로우 수를 말하며, 마지막 수행된 '마이크로 배치' 평균 소요시간을 기준으로 계산 됩니다 |\n",
    "| processedRowsPerSecond | 데이터 싱크로 처리되어 저장되는 로우의 비율을 말합니다. `입력 로우의 수 대비 처리하는 로우의 수가 일정하게 낮다면` 지연되고 있다고 말할 수 있습니다 | \n",
    "| sources and sink | 데이터 소스와 싱크에 대한 정보 |\n",
    "\n",
    "> 터미널 화면에서 임의의 문자열을 타이핑 하고, 결과를 터미널에서 확인한 이후에 마지막 상태를 확인합니다\n",
    "\n",
    "```bash\n",
    "notebook     | -------------------------------------------\n",
    "notebook     | Batch: 1\n",
    "notebook     | -------------------------------------------\n",
    "notebook     | +-----+-----+\n",
    "notebook     | | Word|count|\n",
    "notebook     | +-----+-----+\n",
    "notebook     | |world|    1|\n",
    "notebook     | |hello|    1|\n",
    "notebook     | +-----+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "batchId": 0,
       "durationMs": {
        "addBatch": 348,
        "getBatch": 0,
        "latestOffset": 0,
        "queryPlanning": 17,
        "triggerExecution": 425,
        "walCommit": 28
       },
       "id": "f7a1cef9-04bc-4add-8cf1-ba265a8f07b6",
       "name": "wordCount",
       "numInputRows": 0,
       "processedRowsPerSecond": 0,
       "runId": "cf1b94a7-8f5b-4be6-af2c-83ce8e8aa873",
       "sink": {
        "description": "org.apache.spark.sql.execution.streaming.ConsoleTable$@2587af1f",
        "numOutputRows": 0
       },
       "sources": [
        {
         "description": "TextSocketV2[host: localhost, port: 9999]",
         "endOffset": -1,
         "numInputRows": 0,
         "processedRowsPerSecond": 0,
         "startOffset": null
        }
       ],
       "stateOperators": [
        {
         "customMetrics": {
          "loadedMapCacheHitCount": 0,
          "loadedMapCacheMissCount": 0,
          "stateOnCurrentVersionSizeBytes": 320
         },
         "memoryUsedBytes": 1040,
         "numRowsTotal": 0,
         "numRowsUpdated": 0
        }
       ],
       "timestamp": "2021-08-15T17:07:51.654Z"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON(wordCountQuery.lastProgress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 반복적으로 상태를 확인 하는 displayStatus 함수를 생성합니다\n",
    "\n",
    "```python\n",
    "# 스트림 쿼리의 상태를 주기적으로 조회하는 함수 (name: 이름, query: Streaming Query, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStatus(name, query, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)      # Output Cell 의 내용을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Status: '+query.status['message'])\n",
    "        display(query.lastProgress)  # 마지막 수행된 쿼리의 상태를 출력합니다\n",
    "        sleep(sleep_secs)            # 지정된 시간(초)을 대기합니다\n",
    "        i += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayStatus(name, query, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Status: '+query.status['message'])\n",
    "        display(query.lastProgress)\n",
    "        sleep(sleep_secs)    \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 터미널 환경에서 넷캣(nc) 명령으로 소켓 서버를 띄웁니다\n",
    "\n",
    "```bash\n",
    "nc -lvp 9999 -n\n",
    "Listening on 0.0.0.0 9999\n",
    "```\n",
    "\n",
    "> 서버를 기동하고 Listening 메시지가 뜨면 정상입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[status of query] Iteration: 60, Status: Waiting for next trigger'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '9ea980c7-15f8-404d-92b8-5351dac7140b',\n",
       " 'runId': '2f76de05-aba0-4d5a-a97a-7343f3502dd5',\n",
       " 'name': 'wordCount',\n",
       " 'timestamp': '2021-08-15T16:27:16.001Z',\n",
       " 'batchId': 5,\n",
       " 'numInputRows': 0,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'latestOffset': 0, 'triggerExecution': 0},\n",
       " 'stateOperators': [{'numRowsTotal': 4,\n",
       "   'numRowsUpdated': 0,\n",
       "   'memoryUsedBytes': 3008,\n",
       "   'customMetrics': {'loadedMapCacheHitCount': 15,\n",
       "    'loadedMapCacheMissCount': 5,\n",
       "    'stateOnCurrentVersionSizeBytes': 1176}}],\n",
       " 'sources': [{'description': 'TextSocketV2[host: localhost, port: 9999]',\n",
       "   'startOffset': 0,\n",
       "   'endOffset': 0,\n",
       "   'numInputRows': 0,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 0.0}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@35cbdff9',\n",
       "  'numOutputRows': 0}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!rm -rf $wordCountCheckpointDir\n",
    "wordCountQuery = wordCountTrigger.start()\n",
    "displayStatus(\"status of query\", wordCountQuery, 40, 3)\n",
    "wordCountQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.2 스파크 Web UI 통한 모니터링 하기\n",
    "\n",
    "* 소켓 서버를 통해 작업하기 보다는 파일을 통해 천천히 스트리밍 처리를 하면서 모니터링을 하기 위해 파일을 기반으로 작성합니다\n",
    "\n",
    "#### 활동 로그를 읽어서 주기적으로 스트리밍 처리를 하여 `custom_count` 테이블로 저장합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "customSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"Arrival_Time\", LongType()))\n",
    "    .add(StructField(\"Creation_Time\", LongType()))\n",
    "    .add(StructField(\"Device\", StringType()))\n",
    "    .add(StructField(\"Index\", LongType()))\n",
    "    .add(StructField(\"Model\", StringType()))\n",
    "    .add(StructField(\"User\", StringType()))\n",
    "    .add(StructField(\"gt\", StringType()))\n",
    "    .add(StructField(\"x\", DoubleType()))\n",
    "    .add(StructField(\"y\", DoubleType()))\n",
    "    .add(StructField(\"z\", DoubleType()))\n",
    ")\n",
    "activityPath = f\"{work_data}/activity-data\"\n",
    "customReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(customSchema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .load(activityPath)\n",
    ")\n",
    "customCounter = (\n",
    "    customReader.groupBy(\"gt\").count()\n",
    ")\n",
    "queryName = \"custom_count\"\n",
    "customWriter = (\n",
    "    customCounter\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"update\")\n",
    ")\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "customTrigger = (\n",
    "    customWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $checkpointLocation\n",
    "customQuery = customTrigger.start()\n",
    "customQuery.awaitTermination(80)\n",
    "customQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7f434fce0be0>,\n",
       " <pyspark.sql.streaming.StreamingQuery at 0x7f434fbda730>]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> `http://vm<number>.aiffel.co.kr:4040/jobs` 페이지에 접속하여, 현재 실행 중인 `custom_count` 정보를 확인합니다 (여러개의 SparkContext 가 존재하는 경우 포트가 4041, 4042 로 늘어날 수 있습니다)\n",
    "\n",
    "#### 웹 UI 통한 디버깅\n",
    "  - http://localhost:4040/jobs\n",
    "![spark-streaming-ui](images/spark-streaming-ui.png)\n",
    "\n",
    "#### 개별 스트리밍 쿼리의 상태\n",
    "![spark-streaming-stats](images/spark-streaming-stats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.3 StreamingQueryListeners 객체를 이용하여 매트릭 게시하기\n",
    "\n",
    "> StreamingQueryListener 인터페이스를 통해 다양한 이벤트를 체크할 수 있습니다. 단, `인터페이스 구현을 통한 컴파일 언어만 지원하기 때문에 pyspark 에서는 사용할 수 없습`니다.\n",
    "\n",
    "* 아래의 예제는 3가지 이벤트(시작, 종료, 진행 등)를 모니터링 하는 리스너를 구현합니다\n",
    "```scala\n",
    "import org.apache.spark.sql.streaming._\n",
    "val myListener = new StreamingQueryListener() {\n",
    "    override def onQueryStarted(event: QueryStartedEvent): Unit = {\n",
    "        println(\"Query started: \" + event.id)\n",
    "    }\n",
    "    override def onQueryTerminated(event: QueryTerminatedEvent): Unit = {\n",
    "        println(\"Query terminated: \" + event.id)\n",
    "    }\n",
    "    override def onQueryProgress(event: QueryProgressEvent): Unit = {\n",
    "        println(\"Query made progress: \" + event.progress)\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "* 구현된 리스너를 SparkSession 실행 전에 등록합니다\n",
    "```scala\n",
    "    spark.streams.addListener(myListener)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>1. [중급]</font> 소켓 서버 예제를 `SocketCount` 쿼리라는 이름으로 코딩하고,  Web UI 통하여 상태를 확인하세요\n",
    "\n",
    "> 기존에 수행되고 있는 동일한 애플리케이션 이름이 있다면 실행되지 않으므로 Web UI 에서 확인 후 실행합니다\n",
    "\n",
    "* 소켓 서버 애플리케이션을 기동합니다 (이번 장의 처음 예제 코드를 그대로 사용합니다)\n",
    "  - 호스트 : localhost\n",
    "  - 포트 : 9999\n",
    "* 변환 작업\n",
    "  - 소켓으로 전달 받은 공백으로 구분된 문자열을 단어로 쪼개어(split, explode), \"Word\", \"Count\" 컬럼으로 alias 합니다\n",
    "* 콘솔 싱크\n",
    "  - 쿼리 : SocketCount\n",
    "  - 포맷 : console\n",
    "  - 모드 : complete\n",
    "* 트리거링\n",
    "  - 1초에 한 번 트리거링\n",
    "  - 체크포인트 : /home/jovyan/work/lgde-spark-stream/tmp/wordCount\n",
    "* 애플리케이션\n",
    "  - 타임아웃 : 3분 내외 (소켓 서버 테스트 할 시간)\n",
    "  - 테스트 후 애플리케이션을 종료해 주세요\n",
    "\n",
    "<details><summary>[정답] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었고, Web UI 에서 `SocketCount` 쿼리가 보인다면 성공입니다\n",
    "\n",
    "```python\n",
    "# 아래에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "queryName = \"SocketCount\"\n",
    "socketReader = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).load()\n",
    "socketCounter = socketReader.select(explode(split(col(\"value\"), \"\\s\")).alias(\"word\")).groupBy(\"Word\").count().alias(\"Count\")\n",
    "socketWriter = socketCounter.writeStream.queryName(queryName).format(\"console\").outputMode(\"complete\")\n",
    "wordCountCheckpointDir = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $wordCountCheckpointDir\n",
    "socketTrigger = socketWriter.trigger(processingTime=\"1 second\").option(\"checkpointLocation\", wordCountCheckpointDir)\n",
    "socketQuery = socketTrigger.start()\n",
    "socketQuery.awaitTermination(180)\n",
    "socketQuery.stop()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. 메모리 싱크 테이블을 통한 조회 실습\n",
    "\n",
    "> 라이브 스트리밍 데이터를 통해 실습 혹은 개발을 하기에는 시뮬레이션이 어렵거나, 원하는 대로 테스트하기 어려운 경우가 많습니다. 이러한 경우 원본 데이터를 파일로 저장하고, 출력을 memory 테이블로 설정하여 임의의 스트리밍 데이터를 실습하고 조회할 수 있습니다\n",
    "\n",
    "### 2.1 JSON 스트리밍을 통해 메모리 싱크 테이블 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))\n"
     ]
    }
   ],
   "source": [
    "# 파일을 읽어서 스키마를 확인하고\n",
    "flightPath = f\"{work_data}/flight-data/json\"\n",
    "flightJson = spark.read.json(flightPath)\n",
    "flightJson.printSchema()\n",
    "\n",
    "# 커스텀 스키마를 생성하고\n",
    "flightSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"DEST_COUNTRY_NAME\", StringType()))\n",
    "    .add(StructField(\"ORIGIN_COUNTRY_NAME\", StringType()))\n",
    "    .add(StructField(\"count\", LongType()))\n",
    ")\n",
    "print(flightSchema)\n",
    "\n",
    "# 데이터 소스를 스트림으로 읽어서\n",
    "flightReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(flightSchema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .load(flightPath)\n",
    ")\n",
    "\n",
    "# 변환 로직을 계산하고\n",
    "flightCounter = flightReader.groupBy(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").agg(sum(\"count\").alias(\"count\"))\n",
    "\n",
    "# 싱크 테이블을 메모리로 설정하고\n",
    "queryName = \"memory_flight\"\n",
    "flightWriter = (\n",
    "    flightCounter\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"update\")\n",
    ")\n",
    "\n",
    "# 트리거 설정을 하고\n",
    "!rm -rf $flightCheckpointLocation\n",
    "flightCheckpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "flightTrigger = (\n",
    "    flightWriter\n",
    "    .trigger(processingTime = \"1 second\")\n",
    "    .option(\"checkpointLocation\", flightCheckpointLocation)\n",
    ")\n",
    "\n",
    "# 애플리케이션을 기동합니다\n",
    "flightQuery = flightTrigger.start()\n",
    "flightQuery.awaitTermination(10)\n",
    "flightQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DEST_COUNTRY_NAME</th><th>ORIGIN_COUNTRY_NAME</th><th>count</th></tr>\n",
       "<tr><td>United States</td><td>United States</td><td>2119795</td></tr>\n",
       "<tr><td>United States</td><td>United States</td><td>1761441</td></tr>\n",
       "<tr><td>United States</td><td>United States</td><td>1418309</td></tr>\n",
       "<tr><td>United States</td><td>United States</td><td>1070857</td></tr>\n",
       "<tr><td>United States</td><td>United States</td><td>700855</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------------+-------+\n",
       "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|  count|\n",
       "+-----------------+-------------------+-------+\n",
       "|    United States|      United States|2119795|\n",
       "|    United States|      United States|1761441|\n",
       "|    United States|      United States|1418309|\n",
       "|    United States|      United States|1070857|\n",
       "|    United States|      United States| 700855|\n",
       "+-----------------+-------------------+-------+"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 애플리케이션은 종료되어도 해당 가상 테이블은 여전히 존재하므로, 확인이 가능합니다\n",
    "memory_flight = spark.sql(f\"select * from {queryName}\")\n",
    "memory_flight.orderBy(desc(\"count\")).limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.2 스트리밍 테이블 조회를 위한 모니터링 함수 작성\n",
    "\n",
    "> 스트리밍 테이블을 주기적으로 조회하는 함수를 생성하여 모니터링 할 수 있습니다\n",
    "\n",
    "#### 2.2.1 쿼리 메소드\n",
    "* activityQuery.explain() : 쿼리의 실행계획을 출력\n",
    "* activityQuery.stop() : 쿼리를 중지합니다\n",
    "* activityQuery.awaitTermination() # 쿼리가 종료(query.stop() or exception)될 때까지 대기합니다\n",
    "\n",
    "#### 2.2.2 쿼리 속성\n",
    "* activityQuery.id : 실행되는 쿼리의 고유식별자 (체크포인트로부터 재시작 되어도 변하지 않음)\n",
    "* activityQuery.runId : 실행중인 쿼리의 고유 식별자 (시작 및 재시작 시에 변경)\n",
    "* activityQuery.name : 자동으로 생성된 혹은 이용자가 명시한 쿼리의 이름 - queryName(\"activity_counts\")\n",
    "* activityQuery.exception : 오류와 함께 종료된 쿼리의 예외 정보\n",
    "* activityQuery.recentProgress : 가장 최근의 쿼리가 수행한 상태를 담고 있는 배열 [StreamingQueryProgress]\n",
    "* activityQuery.lastProgress : 마지막으로 수행한 쿼리의 상태 StreamingQueryProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트림 테이블을 주기적으로 조회하는 함수 (name: 이름, sql: Spark SQL, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStream(name, sql, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)              # 출력 Cell 을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Query: '+sql)\n",
    "        spark.sql(sql).show(truncate=False)  # Spark SQL 을 수행합니다\n",
    "        sleep(sleep_secs)                    # sleep_secs 초 만큼 대기합니다\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[count_of_flight] Iteration: 10, Query: select * from memory_flight'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------------------+-----+\n",
      "|DEST_COUNTRY_NAME       |ORIGIN_COUNTRY_NAME   |count|\n",
      "+------------------------+----------------------+-----+\n",
      "|United States           |India                 |69   |\n",
      "|Costa Rica              |United States         |477  |\n",
      "|Turks and Caicos Islands|United States         |136  |\n",
      "|United States           |Afghanistan           |2    |\n",
      "|United States           |Netherlands           |570  |\n",
      "|Iceland                 |United States         |118  |\n",
      "|Marshall Islands        |United States         |77   |\n",
      "|Luxembourg              |United States         |91   |\n",
      "|El Salvador             |United States         |519  |\n",
      "|Samoa                   |United States         |28   |\n",
      "|Sint Maarten            |United States         |61   |\n",
      "|Hong Kong               |United States         |252  |\n",
      "|Suriname                |United States         |12   |\n",
      "|United States           |Bosnia and Herzegovina|1    |\n",
      "|United States           |Portugal              |104  |\n",
      "|United States           |Guatemala             |333  |\n",
      "|United States           |Venezuela             |341  |\n",
      "|United States           |Chile                 |176  |\n",
      "|United States           |Hong Kong             |293  |\n",
      "|Sweden                  |United States         |65   |\n",
      "+------------------------+----------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "displayStream(\"count_of_flight\", f\"select * from {queryName}\", 5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>2. [중급]</font> f\"{work_data}/activity-data\" 경로의 JSON 파일을 읽고, displayStream 함수를 이용해 모니터링 하세요\n",
    "\n",
    "* 스트림 소스를 이용하여 스트리밍 애플리케이션을 작성합니다\n",
    "  - 소스 : /home/jovyan/work/data/activity-data\n",
    "  - 포맷 : json\n",
    "  - 원본 데이터 파일의 스키마를 그대로 활용합니다\n",
    "* 변환 작업\n",
    "  - 핸드폰 사용 패턴을 나타내는 컬럼('gt')를 기준으로 빈도수('count')를 출력하는 스트리밍 애플리케이션을 구현합니다\n",
    "* 콘솔 싱크\n",
    "  - 쿼리 : memory_activity\n",
    "  - 포맷 : memory\n",
    "  - 모드 : complete\n",
    "* 트리거링\n",
    "  - 1초에 한 번 트리거링\n",
    "  - 체크포인트 : /home/jovyan/work/lgde-spark-stream/tmp/memory_activity\n",
    "* 애플리케이션\n",
    "  - 타임아웃 : 2분 내외 (모든 파일을 읽고 처리할 충분한 시간)\n",
    "  - 테스트 후 애플리케이션을 종료해 주세요\n",
    "* 모니터링\n",
    "  - 조회 : 사용패턴 별 빈도수 (gt, count)\n",
    "  - 정렬 : 빈도수 역순 (count desc)\n",
    "\n",
    "<details><summary>[정답] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었고, Web UI 에서 `SocketCount` 쿼리가 보인다면 성공입니다\n",
    "\n",
    "```python\n",
    "# 아래에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "activityPath = f\"{work_data}/activity-data\"\n",
    "activityJson = spark.read.json(activityPath)\n",
    "activityReader = spark.readStream.schema(activityJson.schema).option(\"maxFilesPerTrigger\", 1).json(activityPath)\n",
    "activityCounter = activityReader.groupBy(\"gt\").count()\n",
    "queryName = \"memory_activity\"\n",
    "activityWriter = activityCounter.writeStream.queryName(queryName).format(\"memory\").outputMode(\"complete\")\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "activityTrigger = activityWriter.trigger(processingTime=\"1 second\").option(\"checkpointLocation\", checkpointLocation)\n",
    "activityQuery = activityTrigger.start()\n",
    "displayStream(\"count_of_activity\", f\"select * from {queryName} order by count desc\", 30, 3)\n",
    "activityQuery.stop()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[count_of_activity] Iteration: 30, Query: select * from memory_activity order by count desc'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|gt        |count  |\n",
      "+----------+-------+\n",
      "|walk      |1060402|\n",
      "|sit       |984714 |\n",
      "|stand     |910783 |\n",
      "|bike      |863710 |\n",
      "|stairsup  |836598 |\n",
      "|null      |835725 |\n",
      "|stairsdown|749059 |\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 아래에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>3. [중급]</font> 스파크 코어 라이브러리를 이용하여 2번 과제의 결과와 일치하는지 여부를 확인하는 코드를 작성하세요\n",
    "\n",
    "* 데이터 소스\n",
    "  - 위치 : /home/jovyan/work/data/activity-data\n",
    "  - 포맷 : json\n",
    "* 데이터 변환\n",
    "  - 조건 : 사용패턴 컬럼(gt) 기준으로 그룹(groupBy)한 빈도(count)\n",
    "\n",
    "<details><summary>[정답] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "# 아래에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "activityPath = f\"{work_data}/activity-data\"\n",
    "activityJson = spark.read.json(activityPath)\n",
    "activityJson.groupBy(\"gt\").count().alias(\"count\").orderBy(desc(\"count\"))\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>gt</th><th>count</th></tr>\n",
       "<tr><td>walk</td><td>1060402</td></tr>\n",
       "<tr><td>sit</td><td>984714</td></tr>\n",
       "<tr><td>stand</td><td>910783</td></tr>\n",
       "<tr><td>bike</td><td>863710</td></tr>\n",
       "<tr><td>stairsup</td><td>836598</td></tr>\n",
       "<tr><td>null</td><td>835725</td></tr>\n",
       "<tr><td>stairsdown</td><td>749059</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-------+\n",
       "|        gt|  count|\n",
       "+----------+-------+\n",
       "|      walk|1060402|\n",
       "|       sit| 984714|\n",
       "|     stand| 910783|\n",
       "|      bike| 863710|\n",
       "|  stairsup| 836598|\n",
       "|      null| 835725|\n",
       "|stairsdown| 749059|\n",
       "+----------+-------+"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아래에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
