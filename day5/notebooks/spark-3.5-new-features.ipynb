{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8097e8-9684-4992-bc26-b26cd9c2e7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://notebook:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Data Engineer Training Course</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f63f5e886d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "# 코어 스파크 라이브러리를 임포트 합니다\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Data Engineer Training Course\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90fc222f-b54d-42ba-a2a8-24474c4a6f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|0  |\n",
      "+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(0, 10).withColumnRenamed(\"id\", \"num\")\n",
    "df.show(1, False)\n",
    "spark.sql(\"select current_date() as today, current_timestamp() as now\").createOrReplaceTempView(\"current_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a255d08-8b08-40f9-a3cb-dd969fdd0896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|tableName   |isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|         |current_time|true       |\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "show tables\n",
    "\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da68079e-e5be-4103-8630-ab8e8a17c3c8",
   "metadata": {},
   "source": [
    "#### The IDENTIFER Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11be60fe-3552-4af3-9b02-1f7a83efc1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n",
      "+----------+--------------------------+\n",
      "|today     |now                       |\n",
      "+----------+--------------------------+\n",
      "|2025-09-11|2025-09-11 22:00:41.742998|\n",
      "+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\n",
    "    \"select * from IDENTIFIER(:tbl)\",\n",
    "    args = { \"tbl\": \"current_time\" }\n",
    ")\n",
    "df.printSchema()\n",
    "df.show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c46fb29-262d-4a95-832f-74406453122d",
   "metadata": {},
   "source": [
    "#### HyperLogLog Aggregation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d602d718-b84b-4915-9dce-04d4122be80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|hll_sketch_estimate(hll_sketch_agg(col, 12))|\n",
      "+--------------------------------------------+\n",
      "|3                                           |\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT hll_sketch_estimate(hll_sketch_agg(col))\n",
    "FROM VALUES (\"abc\"), (\"def\"), (\"abc\"), (\"ghi\"), (\"abc\") tab(col)\n",
    "\"\"\").show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1f51e0-a6e4-41ea-8eff-dda77f065595",
   "metadata": {},
   "source": [
    "#### New Functions for Manipulating Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "733f1c62-fbb0-4315-a89a-dc190e63968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|array_append(array(1, 2, 3), 4)|\n",
      "+-------------------------------+\n",
      "|[1, 2, 3, 4]                   |\n",
      "+-------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|array_prepend(array(1, 2, 3), 4)|\n",
      "+--------------------------------+\n",
      "|[4, 1, 2, 3]                    |\n",
      "+--------------------------------+\n",
      "\n",
      "+----------------------------------+\n",
      "|array_insert(array(1, 2, 3), 1, 9)|\n",
      "+----------------------------------+\n",
      "|[9, 1, 2, 3]                      |\n",
      "+----------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|array_compact(array(1, NULL, 3))|\n",
      "+--------------------------------+\n",
      "|[1, 3]                          |\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query(sql):\n",
    "    spark.sql(sql).show(1, False)\n",
    "\n",
    "\n",
    "queries = [\n",
    "    \"SELECT array_append(array(1, 2, 3), 4)\"\n",
    "    # , \"SELECT array_append(array(1, 2, 3), 'ABCD')\" # 오류\n",
    "    , \"SELECT array_prepend(array(1, 2, 3), 4)\"\n",
    "    # , \"SELECT array_insert(array(1, 2, 3), 0, 9)\" # 오류\n",
    "    , \"SELECT array_insert(array(1, 2, 3), 1, 9)\"\n",
    "    , \"SELECT array_compact(array(1, NULL, 3))\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53876a2c-d774-49f4-99ac-6cd0e4c1dbaa",
   "metadata": {},
   "source": [
    "### PySpark Features\n",
    "#### 1. PySpark Existing UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74bea227-8efb-41ce-a3f4-baf21b8be352",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select 2 as value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "442c8523-3401-47cd-9a5d-e0f2b8f20b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|value|square|\n",
      "+-----+------+\n",
      "|2    |4     |\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "square_udf = udf(square, IntegerType())\n",
    "df.withColumn(\"square\", square(df[\"value\"])).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1fabd2-e9a5-419e-b852-087904c4cb90",
   "metadata": {},
   "source": [
    "#### 2. PySpark New UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f5e5cf-3e2a-49ef-bba3-66696050512d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|value|square|\n",
      "+-----+------+\n",
      "|2    |4     |\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(IntegerType())\n",
    "def square_pandas_udf(x: pd.Series) -> pd.Series:\n",
    "    return x * x\n",
    "\n",
    "df.withColumn(\"square\", square_pandas_udf(df[\"value\"])).show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57349d00-d206-4c97-bad1-a15f34b0a651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|id |tag    |\n",
      "+---+-------+\n",
      "|1  |spark  |\n",
      "|1  |delta  |\n",
      "|1  |iceberg|\n",
      "|2  |kafka  |\n",
      "|2  |hudi   |\n",
      "+---+-------+\n",
      "\n",
      "+---+--------------------------------------+\n",
      "|id |tags_with_len                         |\n",
      "+---+--------------------------------------+\n",
      "|1  |[{spark, 5}, {delta, 5}, {iceberg, 7}]|\n",
      "|2  |[{kafka, 5}, {hudi, 4}]               |\n",
      "+---+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "\n",
    "data = [\n",
    "    (1, [\"spark\", \"delta\", \"iceberg\"]),\n",
    "    (2, [\"kafka\", \"hudi\"]),\n",
    "    (3, []),                  # 빈 배열\n",
    "    (4, None)                 # null\n",
    "]\n",
    "df = spark.createDataFrame(data, \"id INT, tags ARRAY<STRING>\")\n",
    "\n",
    "# explode: 배열 → 행으로 확장\n",
    "out_explode = (df\n",
    "    .select(\"id\", F.explode(\"tags\").alias(\"tag\"))   # null이나 빈 배열은 행이 안 나옵니다.\n",
    ")\n",
    "\n",
    "out_explode.show(truncate=False)\n",
    "# +---+-------+\n",
    "# |id |tag    |\n",
    "# +---+-------+\n",
    "# |1  |spark  |\n",
    "# |1  |delta  |\n",
    "# |1  |iceberg|\n",
    "# |2  |kafka  |\n",
    "# |2  |hudi   |\n",
    "# +---+-------+\n",
    "\n",
    "# 예: tag 길이 집계\n",
    "(out_explode\n",
    " .withColumn(\"len\", F.length(\"tag\"))\n",
    " .groupBy(\"id\")\n",
    " .agg(F.collect_list(F.struct(\"tag\",\"len\")).alias(\"tags_with_len\"))\n",
    " .show(truncate=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9577e70-6817-4efa-b960-e5c3cdc95800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udtf\n",
    "\n",
    "@udtf(returnType=\"tag STRING\")\n",
    "class EmitTags:\n",
    "    def eval(self, tags: list[str] | None):\n",
    "        if not tags:    # None / [] 처리\n",
    "            return\n",
    "        for t in tags:\n",
    "            yield (t,)  # 스키마와 동일한 튜플\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1f447a7-4098-4238-aab8-45dc194e94ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@udtf(returnType=\"tag STRING, len INT\")\n",
    "class EmitTagsWithLen:\n",
    "    def eval(self, tags: list[str] | None):\n",
    "        if not tags:\n",
    "            return\n",
    "        for t in tags:\n",
    "            yield (t, len(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6c81533-4135-46d1-baa0-681502588da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+---+-----------------------+\n",
      "|id |tags                   |\n",
      "+---+-----------------------+\n",
      "|1  |[spark, delta, iceberg]|\n",
      "|2  |[kafka, hudi]          |\n",
      "|3  |[]                     |\n",
      "|4  |NULL                   |\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e5e770f-16c6-4cf7-af70-45cf7030f5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current DB: default\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udtf\n",
    "\n",
    "@udtf(returnType=\"tag STRING\")\n",
    "class EmitTags:\n",
    "    def eval(self, tags: list[str] | None):\n",
    "        if not tags:\n",
    "            return\n",
    "        for t in tags:\n",
    "            yield (t,)\n",
    "\n",
    "@udtf(returnType=\"tag STRING, len INT\")\n",
    "class EmitTagsWithLen:\n",
    "    def eval(self, tags: list[str] | None):\n",
    "        if not tags:\n",
    "            return\n",
    "        for t in tags:\n",
    "            yield (t, len(t))\n",
    "\n",
    "print(\"current DB:\", spark.catalog.currentDatabase())\n",
    "\n",
    "# 등록\n",
    "spark.udtf.register(\"default.emit_tags\", EmitTags)\n",
    "spark.udtf.register(\"default.emit_tags_with_len\", EmitTagsWithLen)\n",
    "\n",
    "df.createOrReplaceTempView(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "feb882ba-caa8-4fea-9b3e-b79922ceed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|current_catalog()|current_database()|\n",
      "+-----------------+------------------+\n",
      "|spark_catalog    |default           |\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.udtf.register(\"spark_catalog.default.emit_tags\", EmitTags)\n",
    "spark.udtf.register(\"spark_catalog.default.emit_tags_with_len\", EmitTagsWithLen)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT current_catalog(), current_schema()\n",
    "\"\"\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a48317ae-3e80-4a98-b39d-2928347877e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|function                                |\n",
      "+----------------------------------------+\n",
      "|emit_tags                               |\n",
      "|emit_tags_with_len                      |\n",
      "|spark_catalog.default.emit_tags         |\n",
      "|spark_catalog.default.emit_tags         |\n",
      "|spark_catalog.default.emit_tags_with_len|\n",
      "|spark_catalog.default.emit_tags_with_len|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show functions\").where(expr(\"function like '%emit%'\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "40b22cd6-0e41-445d-8d7f-b5852da8efd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[ROUTINE_NOT_FOUND] The function `default`.`emit_tags` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema and catalog, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP FUNCTION IF EXISTS.; line 1 pos 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM src s LATERAL VIEW emit_tags(s.tags) t AS tag \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [ROUTINE_NOT_FOUND] The function `default`.`emit_tags` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema and catalog, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP FUNCTION IF EXISTS.; line 1 pos 20"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM src s LATERAL VIEW emit_tags(s.tags) t AS tag \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329db48-c319-4956-90d4-5db9bcf15be8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 스파크 3.5.x 환경에서 정상동작하지 않음\n",
    "\n",
    "spark.sql(\"USE spark_catalog\")\n",
    "\n",
    "spark.sql(\"USE default\")\n",
    "\n",
    "print(\"spark.version =\", spark.version)\n",
    "spark.sql(\"SELECT current_catalog() AS cat, current_schema() AS sch\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"SHOW FUNCTIONS IN default LIKE 'emit_tags*'\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"\"\"SELECT s.id, t.tag\n",
    "FROM src s\n",
    "LATERAL VIEW `spark_catalog`.default.emit_tags(s.tags) t AS tag\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5dcf647e-935d-4cbe-98ac-5b0af5a99749",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AnalyzeArgument' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mReadFromConfigFile\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43m@staticmethod\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43manalyze\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAnalyzeArgument\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mSparkFiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetRootDirectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n",
      "Cell \u001b[0;32mIn[93], line 3\u001b[0m, in \u001b[0;36mReadFromConfigFile\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mReadFromConfigFile\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze\u001b[39m(filename: \u001b[43mAnalyzeArgument\u001b[49m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m      5\u001b[0m             SparkFiles\u001b[38;5;241m.\u001b[39mgetRootDirectory(),\n\u001b[1;32m      6\u001b[0m             filename\u001b[38;5;241m.\u001b[39mvalue), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m AnalyzeResult(from_file(f\u001b[38;5;241m.\u001b[39mread()))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AnalyzeArgument' is not defined"
     ]
    }
   ],
   "source": [
    "class ReadFromConfigFile:\n",
    "    @staticmethod\n",
    "    def analyze(filename: AnalyzeArgument):\n",
    "        with open(os.path.join(\n",
    "            SparkFiles.getRootDirectory(),\n",
    "            filename.value), \"r\") as f:\n",
    "            return AnalyzeResult(from_file(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14605172-b8fa-4f10-aa9c-a041068015e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
