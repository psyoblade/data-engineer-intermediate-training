{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e979ceb3-ea79-4dfa-9a1c-e94e1ef15142",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: grpcio in /usr/local/lib/python3.11/site-packages (1.74.0)\n",
      "Requirement already satisfied: grpcio-status in /usr/local/lib/python3.11/site-packages (1.74.0)\n",
      "Requirement already satisfied: protobuf<7.0.0,>=6.31.1 in /usr/local/lib/python3.11/site-packages (from grpcio-status) (6.32.0)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.5.5 in /usr/local/lib/python3.11/site-packages (from grpcio-status) (1.70.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install grpcio grpcio-status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27ae07d8-1dc9-4553-8827-b454599ad9aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|0  |\n",
      "+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.connect.session import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .remote(\"sc://connect-server:15002\")   # docker 네트워크 서비스명: spark\n",
    "    .appName(\"connect-hello\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df = spark.range(0, 10).withColumnRenamed(\"id\", \"num\")\n",
    "df.show(1, False)\n",
    "spark.sql(\"select current_date() as today, current_timestamp() as now\").createOrReplaceTempView(\"current_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ab7c9-cbe4-4226-a5dc-51e4c88ae976",
   "metadata": {},
   "source": [
    "#### The IDENTIFER Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "040e29e2-643f-425b-b309-9affadc76c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|tableName   |isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|         |current_time|true       |\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "show tables\n",
    "\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71725d9d-407c-4f91-b6fb-c1f4b3b28366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n",
      "+----------+--------------------------+\n",
      "|today     |now                       |\n",
      "+----------+--------------------------+\n",
      "|2025-09-10|2025-09-10 14:46:46.838592|\n",
      "+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\n",
    "    \"select * from IDENTIFIER(:tbl)\",\n",
    "    args = { \"tbl\": \"current_time\" }\n",
    ")\n",
    "df.printSchema()\n",
    "df.show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fcd04b-8175-45ba-9986-941e36aea360",
   "metadata": {},
   "source": [
    "#### HyperLogLog Aggregation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04ce5585-cb52-4483-853e-176f33e5f39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|hll_sketch_estimate(hll_sketch_agg(col, 12))|\n",
      "+--------------------------------------------+\n",
      "|3                                           |\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT hll_sketch_estimate(hll_sketch_agg(col))\n",
    "FROM VALUES (\"abc\"), (\"def\"), (\"abc\"), (\"ghi\"), (\"abc\") tab(col)\n",
    "\"\"\").show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b11fe-5d27-4152-a185-9ae99a2b2c77",
   "metadata": {},
   "source": [
    "#### New Functions for Manipulating Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "046170a8-3533-46a1-a044-b1318cc0bf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|array_append(array(1, 2, 3), 4)|\n",
      "+-------------------------------+\n",
      "|[1, 2, 3, 4]                   |\n",
      "+-------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|array_prepend(array(1, 2, 3), 4)|\n",
      "+--------------------------------+\n",
      "|[4, 1, 2, 3]                    |\n",
      "+--------------------------------+\n",
      "\n",
      "+----------------------------------+\n",
      "|array_insert(array(1, 2, 3), 1, 9)|\n",
      "+----------------------------------+\n",
      "|[9, 1, 2, 3]                      |\n",
      "+----------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|array_compact(array(1, NULL, 3))|\n",
      "+--------------------------------+\n",
      "|[1, 3]                          |\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query(sql):\n",
    "    spark.sql(sql).show(1, False)\n",
    "\n",
    "\n",
    "queries = [\n",
    "    \"SELECT array_append(array(1, 2, 3), 4)\"\n",
    "    # , \"SELECT array_append(array(1, 2, 3), 'ABCD')\" # 오류\n",
    "    , \"SELECT array_prepend(array(1, 2, 3), 4)\"\n",
    "    # , \"SELECT array_insert(array(1, 2, 3), 0, 9)\" # 오류\n",
    "    , \"SELECT array_insert(array(1, 2, 3), 1, 9)\"\n",
    "    , \"SELECT array_compact(array(1, NULL, 3))\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4e6ab-48b3-4b16-b1ac-43e171146d34",
   "metadata": {},
   "source": [
    "### PySpark Features\n",
    "\n",
    "#### 1. PySpark UDF 실행방식 변경\n",
    "> spark-connect 사용 시에는 udf 통한 사용은 문제가 되는 것 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b1d9691-f9ab-44e2-b952-a9ba05420ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "square_udf = udf(square, IntegerType())\n",
    "df = spark.sql(\"select 2 as value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd70da52-f274-4494-8655-29c88ed25246",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "SparkContext or SparkSession should be created first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df.withColumn(\u001b[33m\"\u001b[39m\u001b[33msquare\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43msquare_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m).show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/udf.py:423\u001b[39m, in \u001b[36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(\u001b[38;5;28mself\u001b[39m.func, assigned=assignments)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> Column:\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/udf.py:339\u001b[39m, in \u001b[36mUserDefinedFunction.__call__\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> Column:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     sc = \u001b[43mget_active_spark_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m     profiler_enabled = sc._conf.get(\u001b[33m\"\u001b[39m\u001b[33mspark.python.profile\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     memory_profiler_enabled = sc._conf.get(\u001b[33m\"\u001b[39m\u001b[33mspark.python.profile.memory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/utils.py:248\u001b[39m, in \u001b[36mget_active_spark_context\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    246\u001b[39m sc = SparkContext._active_spark_context\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSparkContext or SparkSession should be created first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc\n",
      "\u001b[31mRuntimeError\u001b[39m: SparkContext or SparkSession should be created first."
     ]
    }
   ],
   "source": [
    "df.withColumn(\"square\", square_udf(df[\"value\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82fa1fe7-38fc-4de6-af06-6abf46b4cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.makedirs(\"/data/input\", exist_ok=True)\n",
    "pdf = pd.DataFrame({\n",
    "    \"user_id\": [1,2,3,4,5,6,7,8,9,10],\n",
    "    \"age\": [23,45,31,55,29,40,33,27,48,36],\n",
    "    \"city\": [\"Seoul\",\"Busan\",\"Seoul\",\"Incheon\",\"Daegu\",\"Seoul\",\"Busan\",\"Daejeon\",\"Gwangju\",\"Seoul\"],\n",
    "    \"amount\": [100,200,150,300,120,180,220,80,90,260]\n",
    "})\n",
    "pdf.to_csv(\"/data/input/users.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b9665e-1ab3-4062-a166-09d61b23795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n",
      "+-------+---+-------+------+\n",
      "|user_id|age|   city|amount|\n",
      "+-------+---+-------+------+\n",
      "|      1| 23|  Seoul|   100|\n",
      "|      2| 45|  Busan|   200|\n",
      "|      3| 31|  Seoul|   150|\n",
      "|      4| 55|Incheon|   300|\n",
      "|      5| 29|  Daegu|   120|\n",
      "+-------+---+-------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [sum(amount)#174L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(sum(amount)#174L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=146]\n",
      "      +- HashAggregate(keys=[city#167], functions=[sum(amount#168)])\n",
      "         +- Exchange hashpartitioning(city#167, 200), ENSURE_REQUIREMENTS, [plan_id=143]\n",
      "            +- HashAggregate(keys=[city#167], functions=[partial_sum(amount#168)])\n",
      "               +- Filter (isnotnull(amount#168) AND (amount#168 >= 150))\n",
      "                  +- FileScan csv [city#167,amount#168] Batched: false, DataFilters: [isnotnull(amount#168), (amount#168 >= 150)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/input/users.csv], PartitionFilters: [], PushedFilters: [IsNotNull(amount), GreaterThanOrEqual(amount,150)], ReadSchema: struct<city:string,amount:int>\n",
      "\n",
      "\n",
      "None\n",
      "+-------+-----------+\n",
      "|   city|sum(amount)|\n",
      "+-------+-----------+\n",
      "|  Seoul|        590|\n",
      "|  Busan|        420|\n",
      "|Incheon|        300|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(\"/data/input/users.csv\")\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# 필터 + 집계 + explain\n",
    "res = (df\n",
    "       .select(\"city\", \"amount\")\n",
    "       .where(\"amount >= 150\")\n",
    "       .groupBy(\"city\")\n",
    "       .sum(\"amount\")\n",
    "       .orderBy(\"sum(amount)\", ascending=False))\n",
    "\n",
    "print(res.explain())   # 실행 계획 확인 (필터/프로젝션 푸시다운, AQE 등 확인)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3b127f-6314-4a6c-aeb3-6f1b4a77bd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 읽기(RO)\n",
    "df = (\n",
    "    spark.read.option(\"header\", True).option(\"inferSchema\", True)\n",
    "    .csv(\"file:///data/input/users.csv\")\n",
    ")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cf7b654-f29b-48f2-a930-4819c6286e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- sum(amount): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 처리 & 쓰기(RW, 서버만 씀)\n",
    "out_path = \"file:///data/output/users_by_city\"\n",
    "(\n",
    "    df.groupBy(\"city\").sum(\"amount\")\n",
    "   .write.mode(\"overwrite\").parquet(out_path)\n",
    ")\n",
    "output = spark.read.parquet(out_path)\n",
    "output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77166d6b-3065-456c-bf20-68eb2159fb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   city|avg_amt|\n",
      "+-------+-------+\n",
      "|Gwangju|   90.0|\n",
      "|  Daegu|  120.0|\n",
      "|Incheon|  300.0|\n",
      "|  Busan|  210.0|\n",
      "|Daejeon|   80.0|\n",
      "|  Seoul|  172.5|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테이블로도 저장 (서버 전용 웨어하우스)\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo\")\n",
    "(\n",
    "    df.write.mode(\"overwrite\").saveAsTable(\"demo.users\")\n",
    ")\n",
    "spark.sql(\"SELECT city, AVG(amount) avg_amt FROM demo.users GROUP BY city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a077ff-e780-4c94-8fd4-ce1e47e410de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
